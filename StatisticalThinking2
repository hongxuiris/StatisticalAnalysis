
### the ecdf function takes a 1D array of data as input and returns the x and y values of the ECDF
def ecdf(data):
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) /n
    return x, y
    

## computing the Pearson correlation coefficient
def pearson_r(x, y):
    """ 
    Compute Pearson correlation coefficient between two arrays
    """
    corr_mat = np.corrcoef(x,y)
    return corr_mat[0, 1]


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 1: parameter estimation by optimization

#### Chapter 1.1 optimal parameters: parameter values that bring the model in closest agreement with the data
# the parameters are only optimal for the right model
# packages to do statistical inference: (1) scipy.stats, (2) statsmodels, (3) hacker statistics with NumPy

# finding the optimal parameters by computing the mean and standard deviation from the data
import numpy as np
import matplotlib.pyplot as plt
mean = np.mean(michelson_speed_of_light)
std = np.std(michelson_speed_of_light)
samples = np.random.normal(mean, std, size = 10000)

# how often do we get no-hitters
# nohitter_times: times between no hitters (Poisson process) is exponentially distributed. parameter is time interval tau
np.random.seed(42)
tau = np.mean(nohitter_times)
inter_nohitter_time = np.random.exponential(tau, 100000)
#plot the PDF
_ = plt.hist(inter_nohitter_time, normed = True, histtype='step', bins = 50)
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')
plt.show()

# how is this data optimal? compare with parameter 2*tau and 1/2*tau
plt.plot(x_theor, y_theor)
plt.plot(x,y, marker = '.', linestyle = 'none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

samples_half = np.random.exponential(tau/2, size = 100000)
samples_double = np.random.exponential(2*tau, size = 100000)
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)
plt.show()   # the value of tau given by the mean matches the data best, tau is an optimal paramter

#### Chapter 1.2 linear regression by least squares
# sometimes two variables are related, Pearson correlation coefficient
# a fuller understanding of how the data are related to each other: some underlying function gives the data its shape
# linear regression: use a linear function to describe the relations between variables
# parameters: 
# slope - the slope sets how steep the line is. 
# intercept - the intercept sets where the line crosses the y-axis

# optimal parameter: choose the slope and intercept such that the data points collectively lie as close as possible to the regression line
# residual: the vertical distance between the data point and the regression line
# least squares: the process of finding the parameters for which the sum of the squares of the residual is minimal

slope, intercept = np.polyfit(total_votes, dem_share, 1) # x, y, the degree of the polynomial you wish to fit (for linear function, it's 1)
#slope: we get 'slope' more y for every 1 unit of increase in x

# exercise: the correlation between illiteracy and fertility (average # of children born per woman)
# step 1: EDA - scatter plot and pearson correlation coefficient

_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')
plt.show()

print(pearson_r(illiteracy, fertility))

# linear regression: assume that fertility is a linear function of the female illiteracy rate. That is, f=ai+b, where a is the slope 
# and b is the intercept. We can think of the intercept as the minimal fertility rate. The slope tells us how the fertility rate 
# varies with illiteracy.

_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

a, b = np.polyfit(illiteracy, fertility, 1)
print('slope = ', a, 'children per woman / percent illiterate')
print('intercept = ', b, 'children per woman')

# make theoretical line to plot 
x = np.array([0, 100])
y = a*x + b
_ = plt.plot(x, y)
plt.show()

# how is it optimal? plot RSS (residual sum of squares) vs different slopes
a_vals = np.linspace(0, 0.1, 200) # sploes to consider
rss = np.empty_like(a_vals)  # initialize rss
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy -b) ** 2)
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')
plt.show()


#### Chapter 1.1 **** Do graphical EDA first (after getting data imported and cleaned)****
# any outliers: look into what is causing that outlier
# x is not spreaded across the range: try to acquire more data for intermediate x values
# nonlinear relations: linear model does not fit, choose nonlinear models

# Linear regression on all Anscombe data
anscombe_x = [x1, x2, x3, x4]
anscombe_y = [y1, y2, y3, y4]  # for example, x2 and y2 are the x and y values for the second Anscombe data set.
for x, y in zip(anscombe_x, anscombe_y):
    a, b = np.polyfit(x,y,1)
    print('slope:', a, 'intercept:', b)


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 2: Bootstrap confidence intervals

#### Chapter 2.1 generating boostrap replicates
# we are not interested in the result from a single experiment or data acquisiton 
# simulate getting the data again with hackers: resample the data we have and recompute the summary statistic of interest
# sample with replacement, get a large number of summary statistics from resampled data sets and plot the ECDF

# boostrapping: the use of resampled data to perform statistical inference
# boostrap sample: a resampled array of the data
# boostrp replicate: a statistic computed from a resampled array

import numpy as np
np.random.seed(42)
np.random.choice([1,2,3,4,5], size = 5)
bs_sample = np.random.choice(michelson_speed_of_light, size = 100)

# visuallize boostrap sample with ECDF to get an idea of how the data is distributed
for _ in range(5):
    ba_sample = np.random.choice(rainfall, size = len(rainfall))
    x,y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker = '.', linestyle = 'none', color = 'gray', alpha = 0.1)
x, y = ecdf(rainfall)
_ = plt.plot(x,y, marker = '.')
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()


#### Chapter 2.2 boostrap confidence intervals
# a function to generate a boostrap replicate (1d: because it works on one dimentional arrays)
def boostrap_replicte_1d(data, func):
    """ generate boostrap replicate of 1D data. """
    be_sample = np.random.choice(data, len(data))
    return func(bs_sample)

bs_replicates = np.empty(10000)
for i in range(10000):
    bs_replicates[i] = boostrap_replicate_1d(michelson_speed_of_light, np.mean)
_ = plt.hist(bs_replicates, bins=30, normed = True) #normed = True so the total area of the bars is equal to one
_ = plt.xlabel('mean speed of light (km/s)')
_ = plt.ylabel('PDF')
plt.show()

# confidence interval: quantify the uncertainty about the parameter estimates
# if we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval
# 95% boostrap confidence interval
conf_int = np.percentile(bs_replicates, [2.5, 97.5])
 
## useful function that generates many boostrap replicates
def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))
    
def draw_bs_reps(data, func, size = 1):
    """ Draw boostrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = boostrap_replicate_1d(data, func)
    return bs_replicates

# boostrap replicates of the mean and the SEM (standard error of the mean)
# central limit theorem: the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger 
# â€” no matter what the shape of the population distribution.
# the standard deviation of the sample mean, called the standard error of the mean, or SEM is given by the standard deviation 
# of the data divided by the square root of the number of data points.

bs_replicates = draw_ba_reps(rainfall, np.mean, size = 10000)
sem = np.std(rainfall) /np.sqrt(len(rainfall))
print(sem)
bs_std = np.std(bs_replicates)
print(bs_std)

_ = plt.hist(bs_replicates, bins = 50, normed = True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')
plt.show()

# 95% confidence interval 
np.percentile(bs_replicates, [2.5, 97.5])

###########################################
# boostrap replicates of other statistics (variance), not necessarily Normally distributed
bs_replicates = draw_bs_reps(rainfall, np.var, size = 10000)
bs_replicates /= 100 # put the variance in units of square centimeters
_ = plt.hist(bs_replicates, normed = True, bins = 50)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')
plt.show()  # not Normally distributed as it has a longer tail to the right
# 95% confidence interval for the variance
np.percentile(bs_replicates, [2.5, 97.5])

###############################################
# confidence interval on the rate of no-hitters (Poisson process: Exponentially distributed)
# sample mean (the tau) is Normally distributed (central limit theorem)

# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, size= 10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Print the confidence interval
print('95% confidence interval =', conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')
plt.show()


#### Chapter 2.3 pairs bootstrap
# nonparametric inference: make no assumptions about the model or probability distribution underlying the data

# pairs boostrap for linear regression 
# (1) resample data in pairs
# (2) compute slope and intercept from resampled data
# (3) each slope and intercept is a boostrap replicate
# (4) compute confidence intervals from percentiles of boostrap replicates

inds = np.arange(len(total_votes))
bs_inds = np.random.choice(inds, len(inds))
bs_total_votes = total_votes[bs_inds]
bs_dem_share = dem_share[bs_inds]

bs_slope, bs_intercept = np.polyfit(bs_total_votes, bs_dem_share, 1)
bs_slope, bs_intercept
np.polyfit(total_votes, dem_share, 1) # fit of original
 
# A function to do pairs boostrap
def draw_bs_pairs_linreg(x, y, size=1):
    """ perform pairs boostrap for linear regression """
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size = len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps
    
## Generate replicates of slope and intercept using pairs bootstrap   
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size = 1000)
# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5, 97.5]))
_ = plt.hist(bs_slope_reps, bins=50, normed=True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')
plt.show()    
 
# plotting boostrap regession: plot the line you would get from each boostrap replicate of the slope and intercept (first 100)
x = np.array([0, 100])

for i in range(100):
    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], 
                 linewidth = 0.5, alpha = 0.2, color = 'red')
# plot the data
_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)
plt.show()
 
### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 3: introduction to hypothesis testing

#### Chapter 3.1 formulating and simulating hypothesis
# hypothesis testing: assessment of how reasonable the observed data are assuming a hypothesis is true
# null hypothesis: the hypothesis you are testing
# null hypothesis: county level voting in Ohio and Pennsylvania have identical probability distributions
# by ECDF?, mean, median, standard deviation? we can't really tell is the difference significant or not
# solution: simulate what the data would look like if the two variables were identically distributed
# permutation: random reordering of entries in an array. it's at the heart of simulating a null hypothesis where we assume two
# quantities are identically distributed

import numpy as np
np.random.seed(42)
dem_share_both = np.concatenate((dem_share_PA, dem_share_OH))
dem_share_perm = np.random.permutation(dem_share_both)
perm_sample_PA = dem_share_perm[:len(dem_share_PA)] 
perm_sample_OH = dem_share_perm[len(dem_share_PA):] 
 
def permutation_sample(data1, data2):
    """ Generate a permutation sample from two data sets. """
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2
    
# Visualizing permutation sampling. 
# To see how permutation sampling works, take permutation samples to see how their ECDFs would look if they were identically distributed.
# If none of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis 
# is not commensurate with the data. The two variables are not identically distributed

for _ in range(50):
    perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)
    _ = plt.plot(x_1, y_1, marker = '.', linestyle = 'none', color ='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker = '.', linestyle = 'none', color ='blue', alpha=0.02)

x_1, y_1 = ecdf(rain_june)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()


#### Chapter 3.2 test statistics and p-values
# null hypothesis: county level voting in Ohio and Pennsylvania have identical probability distributions
# test statistic
# (1) a single number that can be computed from observed data and from data you simulate under the null hypothesis
# (2) it serves as a basis of comparison between what the hypothesis predicts and what we actually observed
# (3) choose test statistic to be something pertinent to the question you are trying to answer with your hypothesis test
# for this questin: choose the differnce in mean vote share to be the test statistic

# p-value: the probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the 
# assumption the null hypothesis is true
# statistical significance: determined by the smallness of a p-value
# !!! statistical significance != practical significance
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)
np.mean(dem_share_PA) - np.mean(dem_share_OH)














