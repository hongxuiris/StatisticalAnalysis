
### the ecdf function takes a 1D array of data as input and returns the x and y values of the ECDF
def ecdf(data):
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) /n
    return x, y
    

## computing the Pearson correlation coefficient
def pearson_r(x, y):
    """ 
    Compute Pearson correlation coefficient between two arrays
    """
    corr_mat = np.corrcoef(x,y)
    return corr_mat[0, 1]


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 1: parameter estimation by optimization

#### Chapter 1.1 optimal parameters: parameter values that bring the model in closest agreement with the data
# the parameters are only optimal for the right model
# packages to do statistical inference: (1) scipy.stats, (2) statsmodels, (3) hacker statistics with NumPy

# finding the optimal parameters by computing the mean and standard deviation from the data
import numpy as np
import matplotlib.pyplot as plt
mean = np.mean(michelson_speed_of_light)
std = np.std(michelson_speed_of_light)
samples = np.random.normal(mean, std, size = 10000)

# how often do we get no-hitters
# nohitter_times: times between no hitters (Poisson process) is exponentially distributed. parameter is time interval tau
np.random.seed(42)
tau = np.mean(nohitter_times)
inter_nohitter_time = np.random.exponential(tau, 100000)
#plot the PDF
_ = plt.hist(inter_nohitter_time, normed = True, histtype='step', bins = 50)
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')
plt.show()

# how is this data optimal? compare with parameter 2*tau and 1/2*tau
plt.plot(x_theor, y_theor)
plt.plot(x,y, marker = '.', linestyle = 'none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

samples_half = np.random.exponential(tau/2, size = 100000)
samples_double = np.random.exponential(2*tau, size = 100000)
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)
plt.show()   # the value of tau given by the mean matches the data best, tau is an optimal paramter

#### Chapter 1.2 linear regression by least squares
# sometimes two variables are related, Pearson correlation coefficient
# a fuller understanding of how the data are related to each other: some underlying function gives the data its shape
# linear regression: use a linear function to describe the relations between variables
# parameters: 
# slope - the slope sets how steep the line is. 
# intercept - the intercept sets where the line crosses the y-axis

# optimal parameter: choose the slope and intercept such that the data points collectively lie as close as possible to the regression line
# residual: the vertical distance between the data point and the regression line
# least squares: the process of finding the parameters for which the sum of the squares of the residual is minimal

slope, intercept = np.polyfit(total_votes, dem_share, 1) # x, y, the degree of the polynomial you wish to fit (for linear function, it's 1)
#slope: we get 'slope' more y for every 1 unit of increase in x

# exercise: the correlation between illiteracy and fertility (average # of children born per woman)
# step 1: EDA - scatter plot and pearson correlation coefficient

_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')
plt.show()

print(pearson_r(illiteracy, fertility))

# linear regression: assume that fertility is a linear function of the female illiteracy rate. That is, f=ai+b, where a is the slope 
# and b is the intercept. We can think of the intercept as the minimal fertility rate. The slope tells us how the fertility rate 
# varies with illiteracy.

_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

a, b = np.polyfit(illiteracy, fertility, 1)
print('slope = ', a, 'children per woman / percent illiterate')
print('intercept = ', b, 'children per woman')

# make theoretical line to plot 
x = np.array([0, 100])
y = a*x + b
_ = plt.plot(x, y)
plt.show()

# how is it optimal? plot RSS (residual sum of squares) vs different slopes
a_vals = np.linspace(0, 0.1, 200) # sploes to consider
rss = np.empty_like(a_vals)  # initialize rss
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy -b) ** 2)
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')
plt.show()


#### Chapter 1.3 **** Do graphical EDA first (after getting data imported and cleaned)****
# any outliers: look into what is causing that outlier
# x is not spreaded across the range: try to acquire more data for intermediate x values
# nonlinear relations: linear model does not fit, choose nonlinear models

# Linear regression on all Anscombe data
anscombe_x = [x1, x2, x3, x4]
anscombe_y = [y1, y2, y3, y4]  # for example, x2 and y2 are the x and y values for the second Anscombe data set.
for x, y in zip(anscombe_x, anscombe_y):
    a, b = np.polyfit(x,y,1)
    print('slope:', a, 'intercept:', b)


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 2: Bootstrap confidence intervals

#### Chapter 2.1 generating boostrap replicates
# we are not interested in the result from a single experiment or data acquisiton 
# simulate getting the data again with hackers: resample the data we have and recompute the summary statistic of interest
# sample with replacement, get a large number of summary statistics from resampled data sets and plot the ECDF

# boostrapping: the use of resampled data to perform statistical inference
# boostrap sample: a resampled array of the data
# boostrp replicate: a statistic computed from a resampled array

import numpy as np
np.random.seed(42)
np.random.choice([1,2,3,4,5], size = 5)
bs_sample = np.random.choice(michelson_speed_of_light, size = 100)

# visuallize boostrap sample with ECDF to get an idea of how the data is distributed
for _ in range(50):
    ba_sample = np.random.choice(rainfall, size = len(rainfall))
    x,y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker = '.', linestyle = 'none', color = 'gray', alpha = 0.1)
x, y = ecdf(rainfall)
_ = plt.plot(x,y, marker = '.')
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()


#### Chapter 2.2 boostrap confidence intervals
# a function to generate a boostrap replicate (1d: because it works on one dimentional arrays)
def boostrap_replicte_1d(data, func):
    """ generate boostrap replicate of 1D data. """
    be_sample = np.random.choice(data, len(data))
    return func(bs_sample)

bs_replicates = np.empty(10000)
for i in range(10000):
    bs_replicates[i] = boostrap_replicate_1d(michelson_speed_of_light, np.mean)
_ = plt.hist(bs_replicates, bins=30, normed = True) #normed = True so the total area of the bars is equal to one
_ = plt.xlabel('mean speed of light (km/s)')
_ = plt.ylabel('PDF')
plt.show()

# confidence interval: quantify the uncertainty about the parameter estimates
# if we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval
# 95% boostrap confidence interval
conf_int = np.percentile(bs_replicates, [2.5, 97.5])
 
## useful function that generates many boostrap replicates
def bootstrap_replicate_1d(data, func):
    return func(np.random.choice(data, size=len(data)))
    
def draw_bs_reps(data, func, size = 1):
    """ Draw boostrap replicates."""
    bs_replicates = np.empty(size)
    for i in range(size):
        bs_replicates[i] = boostrap_replicate_1d(data, func)
    return bs_replicates

# boostrap replicates of the mean and the SEM (standard error of the mean)
# central limit theorem: the sampling distribution of the sample means approaches a normal distribution as the sample size gets larger 
# â€” no matter what the shape of the population distribution.
# the standard deviation of the sample mean, called the standard error of the mean, or SEM is given by the standard deviation 
# of the data divided by the square root of the number of data points.

bs_replicates = draw_bs_reps(rainfall, np.mean, size = 10000)
sem = np.std(rainfall) /np.sqrt(len(rainfall))
print(sem)
bs_std = np.std(bs_replicates)
print(bs_std)

_ = plt.hist(bs_replicates, bins = 50, normed = True)
_ = plt.xlabel('mean annual rainfall (mm)')
_ = plt.ylabel('PDF')
plt.show()

# 95% confidence interval 
np.percentile(bs_replicates, [2.5, 97.5])

###########################################
# boostrap replicates of other statistics (variance), not necessarily Normally distributed
bs_replicates = draw_bs_reps(rainfall, np.var, size = 10000)
bs_replicates /= 100 # put the variance in units of square centimeters
_ = plt.hist(bs_replicates, normed = True, bins = 50)
_ = plt.xlabel('variance of annual rainfall (sq. cm)')
_ = plt.ylabel('PDF')
plt.show()  # not Normally distributed as it has a longer tail to the right
# 95% confidence interval for the variance
np.percentile(bs_replicates, [2.5, 97.5])

###############################################
# confidence interval on the rate of no-hitters (Poisson process: Exponentially distributed)
# sample mean (the tau) is Normally distributed (central limit theorem)

# Draw bootstrap replicates of the mean no-hitter time (equal to tau): bs_replicates
bs_replicates = draw_bs_reps(nohitter_times, np.mean, size= 10000)

# Compute the 95% confidence interval: conf_int
conf_int = np.percentile(bs_replicates, [2.5, 97.5])

# Print the confidence interval
print('95% confidence interval =', conf_int, 'games')

# Plot the histogram of the replicates
_ = plt.hist(bs_replicates, bins=50, normed=True)
_ = plt.xlabel(r'$\tau$ (games)')
_ = plt.ylabel('PDF')
plt.show()


#### Chapter 2.3 pairs bootstrap
# nonparametric inference: make no assumptions about the model or probability distribution underlying the data

# pairs boostrap for linear regression 
# (1) resample data in pairs
# (2) compute slope and intercept from resampled data
# (3) each slope and intercept is a boostrap replicate
# (4) compute confidence intervals from percentiles of boostrap replicates

inds = np.arange(len(total_votes))
bs_inds = np.random.choice(inds, len(inds))
bs_total_votes = total_votes[bs_inds]
bs_dem_share = dem_share[bs_inds]

bs_slope, bs_intercept = np.polyfit(bs_total_votes, bs_dem_share, 1)
bs_slope, bs_intercept
np.polyfit(total_votes, dem_share, 1) # fit of original
 
# A function to do pairs boostrap
def draw_bs_pairs_linreg(x, y, size=1):
    """ perform pairs boostrap for linear regression """
    inds = np.arange(len(x))
    bs_slope_reps = np.empty(size)
    bs_intercept_reps = np.empty(size)
    for i in range(size):
        bs_inds = np.random.choice(inds, size = len(inds))
        bs_x, bs_y = x[bs_inds], y[bs_inds]
        bs_slope_reps[i], bs_intercept_reps[i] = np.polyfit(bs_x, bs_y, 1)
    return bs_slope_reps, bs_intercept_reps
    
## Generate replicates of slope and intercept using pairs bootstrap   
bs_slope_reps, bs_intercept_reps = draw_bs_pairs_linreg(illiteracy, fertility, size = 1000)
# Compute and print 95% CI for slope
print(np.percentile(bs_slope_reps, [2.5, 97.5]))
_ = plt.hist(bs_slope_reps, bins=50, normed=True)
_ = plt.xlabel('slope')
_ = plt.ylabel('PDF')
plt.show()    
 
# plotting boostrap regession: plot the line you would get from each boostrap replicate of the slope and intercept (first 100)
x = np.array([0, 100])

for i in range(100):
    _ = plt.plot(x, bs_slope_reps[i]*x + bs_intercept_reps[i], 
                 linewidth = 0.5, alpha = 0.2, color = 'red')
# plot the data
_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
_ = plt.xlabel('illiteracy')
_ = plt.ylabel('fertility')
plt.margins(0.02)
plt.show()
 
### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 3: introduction to hypothesis testing

#### Chapter 3.1 formulating and simulating hypothesis
# hypothesis testing: assessment of how reasonable the observed data are assuming a hypothesis is true
# null hypothesis: the hypothesis you are testing
# null hypothesis: county level voting in Ohio and Pennsylvania have identical probability distributions
# by ECDF?, mean, median, standard deviation? we can't really tell is the difference significant or not
# solution: simulate what the data would look like if the two variables were identically distributed
# permutation: random reordering of entries in an array. it's at the heart of simulating a null hypothesis where we assume two
# quantities are identically distributed

import numpy as np
np.random.seed(42)
dem_share_both = np.concatenate((dem_share_PA, dem_share_OH))
dem_share_perm = np.random.permutation(dem_share_both)
perm_sample_PA = dem_share_perm[:len(dem_share_PA)] 
perm_sample_OH = dem_share_perm[len(dem_share_PA):] 
 
def permutation_sample(data1, data2):
    """ Generate a permutation sample from two data sets. """
    data = np.concatenate((data1, data2))
    permuted_data = np.random.permutation(data)
    perm_sample_1 = permuted_data[:len(data1)]
    perm_sample_2 = permuted_data[len(data1):]
    return perm_sample_1, perm_sample_2
    
# Visualizing permutation sampling. 
# To see how permutation sampling works, take permutation samples to see how their ECDFs would look if they were identically distributed.
# If none of the ECDFs from the permutation samples overlap with the observed data, suggesting that the hypothesis 
# is not commensurate with the data. The two variables are not identically distributed

for _ in range(50):
    perm_sample_1, perm_sample_2 = permutation_sample(rain_june, rain_november)
    x_1, y_1 = ecdf(perm_sample_1)
    x_2, y_2 = ecdf(perm_sample_2)
    _ = plt.plot(x_1, y_1, marker = '.', linestyle = 'none', color ='red', alpha=0.02)
    _ = plt.plot(x_2, y_2, marker = '.', linestyle = 'none', color ='blue', alpha=0.02)

x_1, y_1 = ecdf(rain_june)
x_2, y_2 = ecdf(rain_november)
_ = plt.plot(x_1, y_1, marker='.', linestyle='none', color='red')
_ = plt.plot(x_2, y_2, marker='.', linestyle='none', color='blue')

# Label axes, set margin, and show plot
plt.margins(0.02)
_ = plt.xlabel('monthly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()


#### Chapter 3.2 test statistics and p-values
# null hypothesis: county level voting in Ohio and Pennsylvania have identical probability distributions
# test statistic
# (1) a single number that can be computed from observed data and from data you simulate under the null hypothesis
# (2) it serves as a basis of comparison between what the hypothesis predicts and what we actually observed
# (3) choose test statistic to be something pertinent to the question you are trying to answer with your hypothesis test
# for this questin: choose the differnce in mean vote share to be the test statistic

# p-value: the probability of obtaining a value of your test statistic that is at least as extreme as what was observed, under the 
# assumption the null hypothesis is true
# statistical significance: determined by the smallness of a p-value
# !!! statistical significance != practical significance
np.mean(perm_sample_PA) - np.mean(perm_sample_OH)
np.mean(dem_share_PA) - np.mean(dem_share_OH)

# a function to generate permutation replicates
# a permutation replicate is a single value of a statistic computed from a permutation sample.

def draw_perm_reps(data_1, data_2, func, size =1):
    """ 
    generate multiple permutation replicates.
    func must be a function that takes two arrays as arguments. In most circumstances, func will be a function you write yourself.
    """
    perm_replicates = np.empty(size)
    for i in range(size):
        # Generate permutation sample
        perm_sample_1, perm_sample_2 = permutation_sample(data_1, data_2)

        # Compute the test statistic
        perm_replicates[i] = func(perm_sample_1, perm_sample_2)

    return perm_replicates

# EDA before hypothesis testing
# check if 'impact_force' for frog A and B have the same distribution
_ = sns.swarmplot('ID', 'impact_force', data =df)
_ = plt.xlabel('frog')
_ = plt.ylabel('impact force (N)')
plt.show()

# permutaion test 
def diff_of_means(data_1, data_2):
    """ difference in means of two arrays. """
    diff = np.mean(data_1) - np.mean(data_2)
    return diff
    
empirical_diff_means = diff_of_means(force_a, force_b)
perm_replicates = draw_perm_reps(force_a, force_b, diff_of_means, size = 10000)
#compute p-value: p
p = np.sum(per_replicates >= empirical_diff_means) / len(perm_replicates)
print('p-value = ', p)

#### Chapter 3.3 boostrap hypothesis tests
# pipeline for hypothesis testing
# (1) clearly state the null hypothesis
# (2) define your test statistic
# (3) generate many sets of simulated data assuming the null hypothesis is true
# (4) compute the test statistic for each simulated data set
# (5) The p-value is the fraction of your simulated data sets for which the test statistic is at least as extreme as for the real data

# (1) null hypothesis: the true mean speed of light in Michelson's experiments was actually Newcomb's reported value
# comparing a data set with a value (one sample test), a permutaion test is not applicable
# (to simulate the null hypothesis) we shift Michelson's data such that its mean now matches Newcomb's value
# then use boostrapping on this shifted data to simulate data acquisition under the null hypothesis

# (2) the test statistic is the mean of the boostrap sample minus Newcomb's value

newcomb_value = 299860
michelson_shifted = michelson_speed_of_light - np.mean(michelson_speed_of_light) + newcomb_value

def diff_from_newcomb(data, newcomb_value = 299860):
    return np.mean(data) - newcomb_value
    
diff_obs = diff_from_newcomb(michelson_speed_of_light)
# (3), (4)
bs_replicates = draw_bs_reps(michelson_shifted, diff_from_newcomb, 10000)
p_value = np.sum(bs_replicates <= diff_observed) / 10000

############## a one sample boostrap hypothesis test
# null hypothesis: the mean strike force of Frog B is equal to that of Frog C (0.55)

# Make an array of translated impact forces: translated_force_b
translated_force_b = force_b - np.mean(force_b) + 0.55
# Take bootstrap replicates of Frog B's translated impact forces: bs_replicates
bs_replicates = draw_bs_reps(translated_force_b, np.mean, 10000)
# Compute fraction of replicates that are less than the observed Frog B force: p
p = np.sum(bs_replicates <= np.mean(force_b)) / 10000
# Print the p-value
print('p = ', p)

################# A two-sample bootstrap hypothesis test for difference of means
# We now want to test the hypothesis that Frog A and Frog B have the same mean impact force, but not necessarily the same distribution, 
# which is also impossible with a permutation test.
# Compute mean of all forces: mean_force

mean_force = np.mean(forces_concat)

# Generate shifted arrays
force_a_shifted = force_a - np.mean(force_a) + mean_force
force_b_shifted = force_b - np.mean(force_b) + mean_force

# Compute 10,000 bootstrap replicates from shifted arrays
bs_replicates_a = draw_bs_reps(force_a_shifted, np.mean, size = 10000)
bs_replicates_b = draw_bs_reps(force_b_shifted, np.mean, size = 10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_replicates_a - bs_replicates_b

# Compute and print p-value: p
p = np.sum(bs_replicates >= (np.mean(force_a) - np.mean(force_b) ))/ 10000
print('p-value =', p)


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 4: hypothesis testing examples

#### Chapter 4.1 A/B testing
# A/B test: used by organizations to see if a strategy change gives a better result
# null hypothesis of an A/B test: the test statistic is impervious to the change
# statistical significance does not mean practical significance

# how many more users click through to the website for the redesign versus the original design of the website
# null hypothesis : the click-through rate is not affected by the redesign (permutaion test is a good choice)

import numpy as np
# clickthrough_A, clickthrough_B: array of 1s and 0s
def diff_frac(data_A, data_B):
    frac_A = np.sum(data_A) / len(data_A)
    frac_B = np.sum(data_B) / len(data_B)
    return frac_B - frac_A

diff_frac_obs = diff_frac(clickthrough_A, clickthrough_B)

perm_replicates = np.empty(10000)
for i in range(10000):
    perm_replicates[i] = permutation_replicate(clickthrough_A, clickthrough_B, diff_frac)
   
p_value = np.sum(perm_replicates >= diff_frac_obs) / 10000

########## A/B test example 1: The vote for the Civil Rights Act in 1964
# 153 House Democrats and 136 Republicans voted yea. However, 91 Democrats and 35 Republicans voted nay. Did party affiliation make 
# a difference in the vote?

# Construct arrays of data: dems, reps
dems = np.array([True] * 153 + [False] * 91)
reps = np.array([True] * 136 + [False] * 35)

def frac_yea_dems(dems, reps):
    """Compute fraction of Democrat yea votes."""
    frac = np.sum(dems) / len(dems)
    return frac

# Acquire permutation samples: perm_replicates
perm_replicates = draw_perm_reps(dems, reps, frac_yea_dems, 10000)

# Compute and print p-value: p
p = np.sum(perm_replicates <= 153/244) / len(perm_replicates)
print('p-value =', p)

########## A/B test example 2: A time-on-website analog
# how much time is spent on the website before and after an ad campaign. 
# The frog tongue force (a continuous quantity like time on the website) is an analog.
# perform an A/B test to determine if these rule changes resulted in a slower rate of no-hitters 
# using the difference in mean inter-no-hitter time as your test statistic.

# Compute the observed difference in mean inter-no-hitter times: nht_diff_obs
nht_diff_obs = np.mean(nht_dead) - np.mean(nht_live)  
# negative value, nht_live is longer then nht_dead, values smaller then nht_diff_obs is more extreme

# Acquire 10,000 permutation replicates of difference in mean no-hitter time: perm_replicates
perm_replicates = draw_perm_reps(nht_dead, nht_live, diff_of_means, 10000)

# Compute and print the p-value: p
p = np.sum(perm_replicates <= nht_diff_obs) / len(perm_replicates)
print('p-val =', p)


#### Chapter 4.2 test of correlation
# Pearson correlation coefficient is a measure of how much of the variability in two variables is due to them being correlated

# hypothesis test of correlation 
# (1) Posit null hypothesis: the two variables are completely uncorrelated
# (2) simulate data assuming null hypothesis is true
# (3) use Pearson correlation coefficient, rou, as test statistic
# (4) compute p-value as fraction of replicates that have rou at least as larger as observed.

# Compute observed correlation: r_obs
r_obs = pearson_r(illiteracy, fertility)

# Initialize permutation replicates: perm_replicates
perm_replicates = np.empty(10000)

# Draw replicates
for i in range(10000):
    # Permute illiteracy measurments: illiteracy_permuted
    illiteracy_permuted = np.random.permutation(illiteracy)

    # Compute Pearson correlation
    perm_replicates[i] = pearson_r(illiteracy_permuted, fertility)

# Compute p-value: p
p = np.sum(perm_replicates >= r_obs)/len(perm_replicates)
print('p-val =', p)

############# Do neonicotinoid insecticides have unintended consequences (difference in mean)?
# EDA: Compute x,y values for ECDFs
x_control, y_control = ecdf(control)
x_treated, y_treated = ecdf(treated)

# Plot the ECDFs
plt.plot(x_control, y_control, marker='.', linestyle='none')
plt.plot(x_treated, y_treated, marker='.', linestyle='none')

plt.margins(0.02)
plt.legend(('control', 'treated'), loc='lower right')
plt.xlabel('millions of alive sperm per mL')
plt.ylabel('ECDF')
plt.show()

# Bootstrap hypothesis test on bee sperm counts
# Compute the difference in mean sperm count: diff_means
diff_means = np.mean(control) - np.mean(treated)

# Compute mean of pooled data: mean_count
mean_count = np.mean(np.concatenate([control, treated]))

# Generate shifted data sets
control_shifted = control - np.mean(control) + mean_count
treated_shifted = treated - np.mean(treated) + mean_count

# Generate bootstrap replicates
bs_reps_control = draw_bs_reps(control_shifted, np.mean, size=10000)
bs_reps_treated = draw_bs_reps(treated_shifted, np.mean, size=10000)

# Get replicates of difference of means: bs_replicates
bs_replicates = bs_reps_control - bs_reps_treated

# Compute and print p-value: p
p = np.sum(bs_replicates >= diff_means) \
            / len(bs_replicates)
print('p-value =', p)

### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 5: putting it all together: a case study

#### Chapter 5.1 Finch beaks and the need for statistics
# graphical and quantitative EDA
# parameter estimation
# confidence interval calculation
# hypothesis testing

# how the beak depth of G.scandens has changed over time?
# (1) EDA of beak depths in 1975 and 2012
# (2) parameter estimates of mean beak depth
# (3) hypothesis test: did the beaks get deeper?

### (1) EDA
# Create bee swarm plot
_ = sns.swarmplot('year', 'beak_depth', data =df)
_ = plt.xlabel('year')
_ = plt.ylabel('beak depth (mm)')
plt.show()

# ECDFs of beak depths. While bee swarm plots are useful, we found that ECDFs are often even better when doing EDA.
# Compute ECDFs
x_1975, y_1975 = ecdf(bd_1975)
x_2012, y_2012 = ecdf(bd_2012)

# Plot the ECDFs
_ = plt.plot(x_1975, y_1975, marker='.', linestyle='none')
_ = plt.plot(x_2012, y_2012, marker='.', linestyle='none')

plt.margins(0.02)
_ = plt.xlabel('beak depth (mm)')
_ = plt.ylabel('ECDF')
_ = plt.legend(('1975', '2012'), loc='lower right')
plt.show()  # the differences are much clearer in the ECDF

### (2) Parameter estimates of beak depths
# Estimate the difference of the mean beak depth of the G. scandens samples from 1975 and 2012 and report a 95% confidence interval.
# Compute the difference of the sample means: mean_diff
mean_diff = np.mean(bd_2012) - np.mean(bd_1975)

# Get bootstrap replicates of means
bs_replicates_1975 = draw_bs_reps(bd_1975, np.mean, size = 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012, np.mean, size = 10000)

# Compute samples of difference of means: bs_diff_replicates
bs_diff_replicates = bs_replicates_2012 - bs_replicates_1975

# Compute 95% confidence interval: conf_int
conf_int = np.percentile(bs_diff_replicates, [2.5, 97.5])

# Print the results
print('difference of means =', mean_diff, 'mm')
print('95% confidence interval =', conf_int, 'mm')

### (3) Hypothesis test: Are beaks deeper in 2012?
# Be careful! The hypothesis we are testing is not that the beak depths come from the same distribution. For that we could use a 
# permutation test. The hypothesis is that the means are equal. To perform this hypothesis test, we need to shift the two data sets 
# so that they have the same mean and then use bootstrap sampling to compute the difference of means.

# Compute mean of combined data set: combined_mean
combined_mean = np.mean(np.concatenate((bd_1975, bd_2012)))

# Shift the samples
bd_1975_shifted = bd_1975 - np.mean(bd_1975) + combined_mean
bd_2012_shifted = bd_2012 - np.mean(bd_2012) + combined_mean

# Get bootstrap replicates of shifted data sets
bs_replicates_1975 = draw_bs_reps(bd_1975_shifted, np.mean, size = 10000)
bs_replicates_2012 = draw_bs_reps(bd_2012_shifted, np.mean, size = 10000)

# Compute replicates of difference of means: bs_diff_replicates
bs_diff_replicates =  bs_replicates_2012 - bs_replicates_1975

# Compute the p-value
p = np.sum(bs_diff_replicates >= mean_diff) / len(bs_diff_replicates)

# Print p-value
print('p =', p)


#### Chapter 5.2 Variation of beak shapes








