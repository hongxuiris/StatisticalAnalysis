
### the ecdf function takes a 1D array of data as input and returns the x and y values of the ECDF
def ecdf(data):
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) /n
    return x, y
    

## computing the Pearson correlation coefficient
def pearson_r(x, y):
    """ 
    Compute Pearson correlation coefficient between two arrays
    """
    corr_mat = np.corrcoef(x,y)
    return corr_mat[0, 1]


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 1: parameter estimation by optimization

#### Chapter 1.1 optimal parameters: parameter values that bring the model in closest agreement with the data
# the parameters are only optimal for the right model
# packages to do statistical inference: (1) scipy.stats, (2) statsmodels, (3) hacker statistics with NumPy

# finding the optimal parameters by computing the mean and standard deviation from the data
import numpy as np
import matplotlib.pyplot as plt
mean = np.mean(michelson_speed_of_light)
std = np.std(michelson_speed_of_light)
samples = np.random.normal(mean, std, size = 10000)

# how often do we get no-hitters
# nohitter_times: times between no hitters (Poisson process) is exponentially distributed. parameter is time interval tau
np.random.seed(42)
tau = np.mean(nohitter_times)
inter_nohitter_time = np.random.exponential(tau, 100000)
#plot the PDF
_ = plt.hist(inter_nohitter_time, normed = True, histtype='step', bins = 50)
_ = plt.xlabel('Games between no-hitters')
_ = plt.ylabel('PDF')
plt.show()

# how is this data optimal? compare with parameter 2*tau and 1/2*tau
plt.plot(x_theor, y_theor)
plt.plot(x,y, marker = '.', linestyle = 'none')
plt.margins(0.02)
plt.xlabel('Games between no-hitters')
plt.ylabel('CDF')

samples_half = np.random.exponential(tau/2, size = 100000)
samples_double = np.random.exponential(2*tau, size = 100000)
x_half, y_half = ecdf(samples_half)
x_double, y_double = ecdf(samples_double)
_ = plt.plot(x_half, y_half)
_ = plt.plot(x_double, y_double)
plt.show()   # the value of tau given by the mean matches the data best, tau is an optimal paramter

#### Chapter 1.2 linear regression by least squares
# sometimes two variables are related, Pearson correlation coefficient
# a fuller understanding of how the data are related to each other: some underlying function gives the data its shape
# linear regression: use a linear function to describe the relations between variables
# parameters: 
# slope - the slope sets how steep the line is. 
# intercept - the intercept sets where the line crosses the y-axis

# optimal parameter: choose the slope and intercept such that the data points collectively lie as close as possible to the regression line
# residual: the vertical distance between the data point and the regression line
# least squares: the process of finding the parameters for which the sum of the squares of the residual is minimal

slope, intercept = np.polyfit(total_votes, dem_share, 1) # x, y, the degree of the polynomial you wish to fit (for linear function, it's 1)
#slope: we get 'slope' more y for every 1 unit of increase in x

# exercise: the correlation between illiteracy and fertility (average # of children born per woman)
# step 1: EDA - scatter plot and pearson correlation coefficient

_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')
plt.show()

print(pearson_r(illiteracy, fertility))

# linear regression: assume that fertility is a linear function of the female illiteracy rate. That is, f=ai+b, where a is the slope 
# and b is the intercept. We can think of the intercept as the minimal fertility rate. The slope tells us how the fertility rate 
# varies with illiteracy.

_ = plt.plot(illiteracy, fertility, marker = '.', linestyle = 'none')
plt.margins(0.02)
_ = plt.xlabel('percent illiterate')
_ = plt.ylabel('fertility')

a, b = np.polyfit(illiteracy, fertility, 1)
print('slope = ', a, 'children per woman / percent illiterate')
print('intercept = ', b, 'children per woman')

# make theoretical line to plot 
x = np.array([0, 100])
y = a*x + b
_ = plt.plot(x, y)
plt.show()

# how is it optimal? plot RSS (residual sum of squares) vs different slopes
a_vals = np.linspace(0, 0.1, 200) # sploes to consider
rss = np.empty_like(a_vals)  # initialize rss
for i, a in enumerate(a_vals):
    rss[i] = np.sum((fertility - a*illiteracy -b) ** 2)
plt.plot(a_vals, rss, '-')
plt.xlabel('slope (children per woman / percent illiterate)')
plt.ylabel('sum of square of residuals')
plt.show()


#### Chapter 1.1 **** Do graphical EDA first (after getting data imported and cleaned)****
# any outliers: look into what is causing that outlier
# x is not spreaded across the range: try to acquire more data for intermediate x values
# nonlinear relations: linear model does not fit, choose nonlinear models

# Linear regression on all Anscombe data
anscombe_x = [x1, x2, x3, x4]
anscombe_y = [y1, y2, y3, y4]  # for example, x2 and y2 are the x and y values for the second Anscombe data set.
for x, y in zip(anscombe_x, anscombe_y):
    a, b = np.polyfit(x,y,1)
    print('slope:', a, 'intercept:', b)


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 2: Bootstrap confidence intervals

#### Chapter 2.1 generating boostrap replicates
# we are not interested in the result from a single experiment or data acquisiton 
# simulate getting the data again with hackers: resample the data we have and recompute the summary statistic of interest
# sample with replacement, get a large number of summary statistics from resampled data sets and plot the ECDF

# boostrapping: the use of resampled data to perform statistical inference
# boostrap sample: a resampled array of the data
# boostrp replicate: a statistic computed from a resampled array

import numpy as np
np.random.seed(42)
np.random.choice([1,2,3,4,5], size = 5)
bs_sample = np.random.choice(michelson_speed_of_light, size = 100)

# visuallize boostrap sample with ECDF to get an idea of how the data is distributed
for _ in range(5):
    ba_sample = np.random.choice(rainfall, size = len(rainfall))
    x,y = ecdf(bs_sample)
    _ = plt.plot(x, y, marker = '.', linestyle = 'none', color = 'gray', alpha = 0.1)
x, y = ecdf(rainfall)
_ = plt.plot(x,y, marker = '.')
plt.margins(0.02)
_ = plt.xlabel('yearly rainfall (mm)')
_ = plt.ylabel('ECDF')
plt.show()


#### Chapter 2.2 boostrap confidence intervals
# a function to generate a boostrap replicate (1d: because it works on one dimentional arrays)
def boostrap_replicte_1d(data, func):
    """ generate boostrap replicate of 1D data. """
    be_sample = np.random.choice(data, len(data))
    return func(bs_sample)

bs_replicates = np.empty(10000)
for i in range(10000):
    bs_replicates[i] = boostrap_replicate_1d(michelson_speed_of_light, np.mean)
_ = plt.hist(bs_replicates, bins=30, normed = True) #normed = True so the total area of the bars is equal to one
_ = plt.xlabel('mean speed of light (km/s)')
_ = plt.ylabel('PDF')
plt.show()

# confidence interval: quantify the uncertainty about the parameter estimates
# if we repeated measurements over and over again, p% of the observed values would lie within the p% confidence interval
# 95% boostrap confidence interval
conf_int = np.percentile(bs_replicates, [2.5, 97.5])
 
## useful function that generates many boostrap replicates
def draw_bs_reps(data, func, size = 1):
    """ Draw boostrap replicates."""
    























