
#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 1 key performace indicators (KPI): measuring business success

# A/B testing: test two or more different ideas against each other, see which one empirically performs better
# running the test in real world, no guessing
# provide accurate answers quickly
# allows companies to rapidly iterate on ideas
# a statistically sound way to establish causal relationships (Randomness helps ensure nothing else is impacting our observed results.)

# where can A/B test be used? users + ideas --> A/B test
# test impact of drugs
# mobile games to incentivize user spending
# subscription services driving user growth

# chapter 1.1 KPI (run A/B test to improve KPI)
# drug remission rate, likelihood of drug side effect, revenue, conversion rate, game play time per user
# identify KPI: experience + domain knowledge + exploratory data analysis
# experience + domain knowledge: what's likely to be an important driver of the business
# exploratory data analysis: uncover relationships that reveal which metrics truly measure these drivers

# example: medication app
# services: 1. paid subscription 2. one-off in-app purchase
# goals/KPI: maintain high conversion rate
# additionally we want to maintain strength in a variety of other business areas as well

import pandas as pd
# load customer demographics
customer_demographics = pd.read_csv('customer_demographics.csv')
print(customer_demographics.head())

# load customer subscriptions (user actions)
customer_subscriptions = pd.read_csv('customer_subscriptions.csv')
print(customer_subscriptions.head())

## focus on the KPI of conversion rate (will consider a variety of others)
# one question in defining KPI: over what interval should we consider the conversion rate?
# one way to decide is to see the generalizability of these statistics across different demographic groups
#  * stable, generalizable KPIs are better than custom KPIs. stability is desired so we don't need custom KPIs for each breakdown
# second is to see if one is more correlated with important factors like retention or spending than the others
# pd.merge(df1, df2) or df1.merge(df2)
sub_data_demo = customer_demographics.merge(customer_subscriptions, how = 'inner', on = ['uid'])  ## inner, outer, left, right
sub_data_demo.head()
# aggregate combined dataset to calculate potential KPIs

## exercises : Loading & examining our data
# user demographics and a set of data relating to in-app purchases for our meditation app.

# Import pandas 
import pandas as pd

# Load the customer_data
customer_data = pd.read_csv('customer_data.csv')
# Load the app_purchases
app_purchases = pd.read_csv('inapp_purchases.csv')

# Print the columns of customer data
print(customer_data.columns)
# Print the columns of app_purchases
print(app_purchases.columns)

# Merge on the 'uid' and 'date' field
uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')

# Examine the results 
print(uid_date_combined_data.head())
print(len(uid_date_combined_data))


######## chapter 1.2 exploratory analysis if KPIs
# which conversion rate metric is the most appropriate
# most company will have many KPIs, each serving a different purpose. conversion rate is just one KPI
# calculating KPI and measure performance across different groups
# DataFrame.groupby(by =None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs)
# DataFrame.agg(func, axis =0, *args, **kwargs)

sub_data_grp = sub_data_demo.groupby(by = ['country', 'device'], axis=0, as_index=False)
sub_data_grp.price.mean()
sub_data_grp.price.agg('mean')
sub_data_grp.price.agg(['mean', 'median'])
sub_data_grp.agg({'price': ['mean', 'min', 'max'], 'age': ['mean', 'min', 'max']})

# custom functions, removes the top and bottom ten percent of values before calculating the mean
def truncated_mean(data):
    top_val = data.quantile(0.9)
    bot_val = data.quantile(0.1)
    trunc_data = data[(data <= top_val) & (data >= bot_val)]
    mean = trunc_data.mean()
    return(mean)
    
sub_data_grp.agg({'age':[truncated_mean]})    # no quotation marks for custom functions

# exploring the in-app purchase data in more detail.
# examine the results to get an understanding of the overall data

# Calculate the mean and median of price and age
purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})
# Examine the output 
# mean is higher than the median? This suggests that we have some users who are making a lot of purchases!
print(purchase_summary)

# calculate a set of summary statistics about the purchase data broken out by 'device' (Android or iOS) and 'gender' (Male or Female).
# Following this, you'll compare the values across these subsets, which will give you a baseline for these values as potential 
# KPIs to optimize going forward.

# These values provide a great summary of the customer data which will be useful as you move to optimizing the conversion rate.
# Group the data 
grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])

# Aggregate the data
purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})

# Examine the results
print(purchase_summary)


######## chapter 1.3 calculating KPIs - a practical example
# comparing KPIs "user conversion rate" after free trial ends
# 1. first week after the trial ends

import numpy as np
from datetime import datetime, timedelta
current_date = pd.to_datetime('2018-03-17')
# "lapse date" = date that the trial ended
print(sub_data_demo.lapse_date.max())  #'2018-03-17'

# remove users whose lapse date with 7 days
max_lapse_date = current_date - timedelta(days=7)
# restrict to users lapsed before max_lapse_date
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]
# count the users
total_users_count = conv_sub_data.price.count()
print(total_users_count)

# latest subscription date: within 7 days of lapsing
max_sub_date = conv_sub_data.lapse_date + timedelta(days = 7)

# filter the users with non-zero subscription price and who subscribed before max_sub_date
total_subs = conv_sub_data[(conv_sub_data.price > 0) & (conv_sub_data.subscription_date <= max_sub_date)]

# count the users
total_subs_count = total_subs.price.count()
print(total_subs_count)

conversion_rate = total_subs_count/total_users_count
print(conversion_rate) #0.2325

### check week one and week two conversion rates across different cohorts
max_lapse_date = current_date - timedelta(days=14)
# restrict to users lapsed before max_lapse_date
#conv_sub_data_7days = conv_sub_data.copy()
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]

# find number of days between subscription and lapse date if subscribed
# ... and pd.NaT otherwise
sub_time = np.where(conv_sub_data.subscription_date.notnull(), (conv_sub_data.subscription_date - \
           conv_sub_data.lapse_date).dt.days, pd.NaN)
conv_sub_data['sub_time'] = sub_time

# gcr7(), gcr14: conversion rates over 7 and 14 days
def gcr7(sub_time):
    return sum(sub_time <= 7) / len(sub_time) # sub_time.count() number of not null values

# form cohorts
purchase_cohorts = conv_sub_data.groupby(by = ['gender, 'device'], as_index=False)
# find group conversion rate for each cohort using gcr7, gcr14
purchase_cohorts.agg({sub_time: [gcr7m, gcr14]})    
# similar convertion rates between cohorts which is good   

### how to choose KPI metrics
# 1. consider how long it takes (monthly conversion rate: impractical to wait for a month on an actionable time scale)
# 2. use exploratory data analysis to reveal relationships between metrics and key results
# 3. explore ties to business metrics

# KPI need to be important. for instance conversion rate is important because it could potentially serve as a warning of potential 
# problems later on
# check how does it evolve over time?
# measuring KPI across groups is crucial because changes can impact groups in drastically different ways

## exercise: Calculating KPIs - average amount paid per purchase within a user's first 28 days
# Compute max_purchase_date
max_purchase_date = current_date - timedelta(days=28)

# Filter to only include users who registered before our max date
purchase_data_filt = purchase_data[purchase_data.reg_date < max_purchase_date]

# Filter to contain only purchases within the first 28 days of registration
purchase_data_filt = purchase_data_filt[(purchase_data_filt.date <= 
                        purchase_data_filt.reg_date + timedelta(days=28))]

# Output the mean price paid per purchase
print(purchase_data_filt.price.mean()) # 414 cents
# The average price is 414 cents which is below $4.99 it seems that our purchasers tend towards the lower priced set of options.

### Average purchase price by cohort

# Set the max registration date to be one month before today
max_reg_date = current_date - timedelta(days=28)

# Find the month 1 values
month1 = np.where((purchase_data.reg_date < max_reg_date) &
                 (purchase_data.date < purchase_data.reg_date + timedelta(days=28)),
                  purchase_data.price, np.NaN)
                 
# Update the value in the DataFrame
purchase_data['month1'] = month1

# Group the data by gender and device 
# We can calculate these metrics across a set of cohorts and see what differences emerge. 
# This is a useful task as it can help us understand how behaviors vary across cohorts.
purchase_data_upd = purchase_data.groupby(by=['gender', 'device'], as_index=False) 

# Aggregate the month1 and price data 
purchase_summary = purchase_data_upd.agg(
                        {'month1': ['mean', 'median'],
                        'price': ['mean', 'median']})  # price: let's look at these metrics not limited to 28 days to compare.

# Examine the results 
print(purchase_summary) # This value seems relatively stable over the past 28 days


#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 2 exploring and visualizing customer behavior

# chapter 2.1 EDA and time series data to uncover trends in KPI
# look at the evolution of KPI over time or time based KPIs

current_date = pd.to_datetime('2018-03-17')
max_lapse_date = current_date - timedelta(days=14)
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]
sub_time = np.where(conv_sub_data.subscription_date.notnull(), (conv_sub_data.subscription_date - \
           conv_sub_data.lapse_date).dt.days, pd.NaN) #.dt.days convert to integer rather than the unit of days
conv_sub_data['sub_time'] = sub_time
# not subscribed in week one (sub_time > 7 or not subscribed yet )
conv_base = conv_sub_data[(conv_sub_data.sub_time.notnull()) | (conv_sub_data.sub_time >7) ]
total_users = len(conv_base)
total_subs = np.where(conv_base.sub_time.notnull() & (conv_base.sub_time <=14), 1, 0)
total_subs = sum(total_subs)
conversion_rate = total_subs / total_users

# pasring date
pd.read_csv("data.csv", parse_dates=True, infer_datetime_format = True)
pd.to_datime(arg, errors = 'raise', ..., format=None, )

# strftime
# 1993-01-27 -- "%Y-%m-%d"
# 05/13/2017 05:45:37 -- "%m/%d/%Y %H:%M:%S"
# September 01, 2017 -- "%B %d, %Y"
# Saturday January 27, 2017 -- '%A %B %d, %Y'
# 2016 March 01 01:56 --"%Y %B %d %H:%M"
date_data_one = pd.to_datetime(date_data_one, format='%A %B %d, %Y')
print(date_data_one)

#### chapter 2.1 creating time series graphs with matplotlib

# conversion rate over time, monitoring the impact of changes
current_date = pd.to_datetime('2018-03-17')
max_lapse_date = current_date - timedelta(days=7)
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]
sub_time = np.where(conv_sub_data.subscription_date.notnull(), (conv_sub_data.subscription_date - \
           conv_sub_data.lapse_date).dt.days, pd.NaN) #.dt.days convert to integer rather than the unit of days
conv_sub_data['sub_time'] = sub_time

conversion_data = conv_sub_data.groupby(by = ['lapse_date'], as_index=False)
conversion_data = conversion_data.agg({'sub_time':[gcr7]})
conversion_data.columns = conversion_data.columns.droplevel(level=1)
conversion_data.head() # week one conversion rate by registration date

# plot daily one-week conversion rate
conversion_data.lapse_date = pd.to_datetime(conversion_data.lapse_date) # 'sub_time' is conversion rate
conversion_data.plot(x='lapse_date', y = 'sub_time')
plt.show()

## further segmenting time series graphs can provide insight into whether these changes are impacting all users equally
# or different cohorts in different ways (splitting by coountry and device)

conversion_data = conv_sub_data.groupby(by = ['lapse_date', 'country'], as_index=False)
reformatted_cntry_data = pd.pivot_table(conversion_data, values = ['sub_time'], columns = ['country'], index = ['reg_date'], fill_value=0)
reformatted_cntry_data.columns = reformatted_cntry_data.columns.droplevel(level=[0])
reformatted_cntry_data.reset_index(inplace=True)
reformatted_cntry_data.head()
reformatted_cntry_data.plot(x='reg_date', y = ['BRA', 'FRA', 'DEU', 'TUR'])
plt.show()

### Plotting time series data: check how price change impact on the number of purchases made by purchasing users during their first week.
# 'first_week_purchases' : 1/0 then converted to the average number of purchases made per day by users in their first week.
# Group the data and aggregate first_week_purchases
user_purchases = user_purchases.groupby(by=['reg_date', 'uid']).agg({'first_week_purchases': ['sum']})

# Reset the indexes
user_purchases.columns = user_purchases.columns.droplevel(level=1)
user_purchases.reset_index(inplace=True)

# Find the average number of purchases per day by first-week users
user_purchases = user_purchases.groupby(by=['reg_date']).agg({'first_week_purchases': ['mean']})
user_purchases.columns = user_purchases.columns.droplevel(level=1)
user_purchases.reset_index(inplace=True)

# Plot the results
user_purchases.plot(x='reg_date', y='first_week_purchases')
plt.show()

######### segment by country 
country_pivot = pd.pivot_table(user_purchases_country, values=['first_week_purchases'], columns=['country'], index=['reg_date'])
print(country_pivot.head())
# Plot the average first week purchases for each country by registration date
country_pivot.plot(x='reg_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])
plt.show()

### techniques to uncover the trends
# subscribers per day
usa_subscriptions = pd.read_csv("usa_subscriptions.csv", parse_dates = True, infer_datetime_format=True)
usa_subscriptions['sub_day'] = (usa_subscriptions.sub_date - usa_subscriptions.lapse_date).dt.days
usa_subscriptions = usa_subscriptions[usa_subscriptions.sub_day <=7]
usa_subscriptions = usa_subscriptions.groupby(by = ['sub_date'], as_index = False)
usa_subscriptions = usa_subscriptions.agg({'subs':['sum']})
usa_subscriptions.columns = usa_subscriptions.columns.droplevel(level=[1])
usa_subscriptions.head()
usa_subscriptions.plot(x='sub_date', y='subs')
plt.show()

#### chapter 2.2 correct seasonality using trailing average, remove data noise by exponential moving average

## seasonality: there is weekly seasonality. seasonality obfuscate macro-level trends
###################  to correct: calculating a trailing average over the data
## trailing average is a smoothing technique that sets the value for a given day as the average over the past n-days
# to smooth weekly seasonality we want n equals 7 (average over a week, limiting the day level effects)
rolling_subs = usa_subscriptions.subs.rolling(window = 7, center = False)
rolling_subs = rolling_subs.mean()
usa_subscriptions['rolling_subs'] = rolling_subs
usa_subscriptions.tail()
usa_subscriptions.plot(x='sub_date', y='rolling_subs') ## xlabel, ylabel
plt.show()

############## noisy data: to correct: exponential moving averages to check any macro trends are hidden among the noise
### exponential moving average weights the points such that the earlier ones are weighted less than the more recent ones within the window.

# purchase per day
high_sku_purchases = pd.read_csv('high_sku_purchases.csv', parse_dates=True, infer_datetime_format = True)
high_sku_purchases.plot(x='date', y='purchases')
plt.show()
exp_mean = high_sku_purchases.purchases.ewm(span=30)  
# span to specify window size, determining window size requires prior knowledge of the structure of the data or some trial and error
exp_mean = exp_mean.mean()
high_sku_purchases['exp_mean'] = exp_mean
high_sku_purchases.plot(x='date', y='exp_mean')
plt.show()


### Seasonality and moving averages: is purchase grow lead to revenue growth (need to remove revenue seasonality to reveal the trend)
# Compute 7_day_rev
daily_revenue['7_day_rev'] = daily_revenue.revenue.rolling(window=7,center=False).mean()

# Compute 28_day_rev
daily_revenue['28_day_rev'] = daily_revenue.revenue.rolling(window=28,center=False).mean()
    
# Compute 365_day_rev
daily_revenue['365_day_rev'] = daily_revenue.revenue.rolling(window=365,center=False).mean()
    
# Plot date, and revenue, along with the 3 rolling functions (in order)    
daily_revenue.plot(x='date', y=['revenue', '7_day_rev', '28_day_rev', '365_day_rev', ])
plt.show()

##Exponential rolling average & over/under smoothing
# our revenue is somewhat flat over time (shown by trailing average). why this is the case?
# look at a single product to see if this potentially reveals any trends.
# As this will have less data then looking at our overall revenue it will be much noisier. 
# To account for this we will smooth the data using an exponential rolling average.

# Calculate 'small_scale'
daily_revenue['small_scale'] = daily_revenue.revenue.ewm(span=10).mean()

# Calculate 'medium_scale'
daily_revenue['medium_scale'] = daily_revenue.revenue.ewm(span=100).mean()

# Calculate 'large_scale'
daily_revenue['large_scale'] = daily_revenue.revenue.ewm(span=500).mean()

# Plot 'date' on the x-axis and, our three averages and 'revenue'
# on the y-axis
daily_revenue.plot(x = 'date', y =['revenue', 'small_scale', 'medium_scale', 'large_scale'])
plt.show()



#### chapter 2.3 exploratory data analysis with time series data
# drop in new user retention 
current_date = pd.to_datetime('2018-03-17')
max_lapse_date = current_date - timedelta(days=7)
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]
sub_time = np.where(conv_sub_data.subscription_date.notnull(), (conv_sub_data.subscription_date - \
           conv_sub_data.lapse_date).dt.days, pd.NaN) #.dt.days convert to integer rather than the unit of days
conv_sub_data['sub_time'] = sub_time

conversion_data = conv_sub_data.groupby(by = ['lapse_date'], as_index=False)
conversion_data = conversion_data.agg({'sub_time':[gcr7]})
conversion_data.columns = conversion_data.columns.droplevel(level=1)
conversion_data.plot()
plt.show()

## limiting our view (to look at the most recent 6 months)
current_date = pd.to_datetime('2018-03-17')
start_date = current_date - timedelta(days = (6*28))
conv_filter = ((conversion_data.lapse_date >= start_date) & (conversion_data.lapse_date <= current_date))
conversion_data_filt = conversion_data[conv_filter]
conversion_data_filt.plot(x='lapse_date', y='sub_time')
plt.show() # there is a drop 

#### is this trend impact overall or perticular segment
# segmenting our graph
# by country
conv_filter = ((conv_sub_data.lapse_date >= start_date) & (conv_sub_data.lapse_date <= current_date))
conv_data = conv_sub_data[conv_filter]
conv_data_cntry = conv_data.groupby(by = ['lapse_date', 'country'], as_index = False)
conv_data_cntry = conv_data_cntry.agg({'sub_time': [gcr7]})
conv_data_cntry.columns = conv_data_cntry.columns.droplevel(level=1)
conv_data_cntry = pd.pivot_table(conv_data_cntry, values=['sub_time'], columns = ['country'], index = ['lapse_date'], fill_value=0
conv_data_cntry.columns = conv_data_cntry.columns.droplevel(level=0)
conv_data_cntry.reset_index(inplace = True)
conv_data_cntry.plot(x='lapse_date', y=['USA', 'CAN', 'FRA', 'BRA', 'TUR', 'DEU'])
plt.show()

# then by device
conv_filter = ((conv_sub_data.lapse_date >= start_date) & (conv_sub_data.lapse_date <= current_date))
conv_data = conv_sub_data[conv_filter]
conv_data_dev = conv_data.groupby(by = ['lapse_date', 'device'], as_index = False)
conv_data_dev = conv_data_dev.agg({'sub_time': [gcr7]})
conv_data_dev.columns = conv_data_dev.columns.droplevel(level=1)
conv_data_dev = pd.pivot_table(conv_data_dev, values=['sub_time'], columns = ['device'], index = ['lapse_date'], fill_value=0
conv_data_dev.columns = conv_data_dev.columns.droplevel(level=0)
conv_data_dev.reset_index(inplace = True)
conv_data_dev.plot(x='lapse_date', y=['iOS', 'and'])
plt.show()

## adding annotations: events (holidays) and releases (software releases)
events = pd.read_csv('events.csv')
events.head()
releases = pd.read_csv('releases.csv')

## Plotting Annotations: overlay to our main plot
conv_data_dev.plot(x='lapse_date', y=['iOS', 'and'])
events.Date = pd.to_datetime(events.Date)
for row in events.iterrows():
    tmp = row[1]
    plt.axvline(x=tmp.Date, color='k', linestyle = '--')

# release annotation
releases.Date = pd.to_datetime(releases.Date)
for row in releases.iterrows():
    tmp = row[1]
    if tmp.Event == 'iOS Release':
        plt.axvline(x=tmp.Date, color='b', linestyle = '--')
    else:
        plt.axvline(x=tmp.Date, color='b', linestyle = '--') 
plt.show()

### exercise -- Visualizing user spending 
# monitor the company revenue. Additionally, the product team believes that some of these changes may impact female users more than 
# male users. plot the monthly revenue for one of the updated products and evaluate the results.
# Pivot user_revenue
pivoted_data = pd.pivot_table(user_revenue, values ='revenue', columns=['device', 'gender'], index='month')
pivoted_data = pivoted_data[1:(len(pivoted_data) -1 )]  
#Remove the first and last row of the DataFrame once pivoted to prevent discontinuities from distorting the results. 

# Create and show the plot
pivoted_data.plot()
plt.show()



#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 3 The design and application of A/B testing

# chapter 3.1  Introduction to A/B testing

# A/B test is an experiment in which you test two different values of the same variable against one another
# randomly assign users to control and test group
# good problems for A/B testing are those where users are being impacted in an individual way
# bad problems for A/B testing are those in which there are network effects of users, dating app

demographics_data = pd.read_csv('user_demographics.csv')
demographics_data.head(4)
paywall_views = pd.read.csv('paywall_views.csv')
paywall_views.head(4)

# response variable: KPI or somthing directly related to KPI, simple to measure
# factors that impact our response, color of paywall
# experiment units: the unit over which metrics are measured before aggregating over the control or treatment group overall, users

purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data_agg = purchase_data.groupby(by = ['uid'], as_index=False)
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
total_purchase.purchase.mean()
min(total_purchase.purchase) #0
max(total_purchase.purchase) #17 varies a lot
# since the amount of time on our platform varies widely between users, so it's not meaningful to compare

# user-days as experiment unit: treat each user's action on a given day as a unique unit

purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data.date = purchase_data.date.dt.floor('d')
purchase_data_agg = purchase_data.groupby(by = ['uid', 'date'], as_index=False)
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
total_purchase.purchase.mean()
min(total_purchase.purchase) #0
max(total_purchase.purchase) #3

## Experimental units: Revenue per user day
# Extract the 'day'; value from the timestamp
purchase_data.date = purchase_data.date.dt.floor('d')

# Replace the NaN price values with 0 
purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)

# Aggregate the data by 'uid' & 'date'
purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)
revenue_user_day = purchase_data_agg.sum()

# Calculate the final average
revenue_user_day = revenue_user_day.price.mean()
print(revenue_user_day)

#########################################################################################
## chapter 3.2 preparing to run an A/B test
# there are 3 different consumable price points (price changed 3 times) which may factor into our test
# main concerns in designing a test
# 1. ensure test can be practically run 2. can derive meaningful results from it

# test sensitivity: what percentage of change would be meaningful to detect in response variable
# exercise: look at what different sensitivity look like for your experiment unit

# 1% , 10%, 20% change in revenue per user-day
revenue_user_day * 1.01 (1.1/1.2)

# it's important to understand the latent variability in the data
# standard deviation
purchase_variation = total_purchase.purchase.std()
# typically we will rely on the standard deviation of the test result in evaluating the test, use the value of the original data for planning
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
avg_purchases = total_purchase.purchase.mean()

purchase_variation / avg_purchases # small is better

#### choosing experimental unit & response variable
purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data_agg = purchase_data.groupby(by = ['uid'], as_index=False)
conversion_rate = (sum(purchase_data.purchase)/purchase_data.purchase.count())
conversion_rate ## this is the baseline

#### Conversion rate sensitivities: to examine what that value becomes under different percentage lifts and look at how many more 
# conversions per day this change would result in.

# Merge and group the datasets
purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])
purchase_data.date = purchase_data.date.dt.floor('d')

# Group and aggregate our combined dataset 
daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)
daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})

# Find the mean of each field and then multiply by 1000 to scale the result ( a sample of 0.1% of our overall population )
daily_purchases = daily_purchase_data.purchase['sum'].mean()
daily_paywall_views = daily_purchase_data.purchase['count'].mean()
daily_purchases = daily_purchases * 1000
daily_paywall_views = daily_paywall_views * 1000

print(daily_purchases)
print(daily_paywall_views)

small_sensitivity = 0.1 (0.2/0.5)

# Find the conversion rate when increased by the percentage of the sensitivity above
small_conversion_rate = conversion_rate * (1 + small_sensitivity) 

# Apply the new conversion rate to find how many more users per day that translates to
small_purchasers = daily_paywall_views * small_conversion_rate

# Subtract the initial daily_purcahsers number from this new value to see the lift
purchaser_lift = small_purchasers - daily_purchases

print(small_conversion_rate)
print(small_purchasers)
print(purchaser_lift)

############ Standard error for a conversion rate
## conversion rate p is normally distributed, SEM is std/sqrt(n) = sqrt( p*(1-p)/ n)
# Find the number of paywall views 
n = purchase_data.purchase.count()

# Calculate the quantitiy "v"
v = conversion_rate * (1 - conversion_rate) 

# Calculate the variance and standard error of the estimate
var = v / n 
se = var**0.5

print(var)
print(se)

#########################################################################################
## chapter 3.3 calculating sample sizes
# type I error: rejecting the null hypothesis when the null hypothesis is true
# type II error: retaining the false null hypothesis
# confidence level : the probability of not making a type I error (the hight cl the larger sample size needed , common 0.95)
# statistical power: the probability of finding statistically significant results when the null hypothesis is false

# power, confidence level, standard error, sensitivity and sample size
# as sample size goes up, so does power
# as confidence level goes up, power goes down

## sample size function 

from scipy import stats

def get_power(n, p1, p2, cl):
    alpha = 1-cl
    qu = stats.norm.ppf(1 - alpha/2) 
    # scipy.stats.norm.ppf compute the inverse of the CDF of the standard normal distribution
    # "percent point function" (ppf) is a terrible name. Most people in statistics just use "quantile function".
    diff = abs(p2-p1)
    se = np.sqrt((p1*(1-p1)+p2*(1-p2))/n)
    beta = stats.norm.cdf(qu*se, diff, se)
    return 1-beta
    
def get_sample_size(power, p1, p2, cl, max_n=1000000):
    n=1
    while n <= max_n:
        tmp_power = get_power(n, p1,p2, cl)
        if tmp_power >=power:
            return n
        else:
            n= n+1  # n = n+100
        
        
conversion_rate = 0.03468
sample_size_per_group = get_sample_size(0.8, conversion_rate, conversion_rate*1.1, 0.95)
print(sample_size_per_group) #45794 VS (45788)

# ppf(q, loc=0, scale=1)  # Percent point function (inverse of cdf — percentiles)  it's Z score for standard normal  
# from scipy.stats import norm
# norm.ppf(.5)  # half of the mass is before zero
# 0.0

#The equivalent of the R pnorm() function is: scipy.stats.norm.cdf() with python 
#The equivalent of the R qnorm() function is: scipy.stats.norm.ppf() with python

#The function pnorm returns the integral from −∞ to q of the pdf of the normal distribution where q is a Z-score. 
#Try to guess the value of pnorm(0). (pnorm has the same default mean and sd arguments as dnorm).
#pnorm(0) # 0.5


###### ways to decrease the needed sample size
# 1. switch the unit of observation in a way that reduces variability in the data,for instance switch from revenue to conversion rate
# 2. exclude users who are irrelevant to the process

#decreasing our confidence level has a slightly larger impact on the power than increasing our sample size

# Merge the demographics and purchase data to only include paywall views
purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])
                            
# Find the conversion rate
conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())

# Desired Power: 0.95
# CL 0.90
# Percent Lift: 0.1
p2 = conversion_rate * (1 + 0.1)
sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)
print(sample_size)



#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 4 analyzing A/B testing results

# chapter 4.1  analyzing the A/B test results

# test results
test_demographics = pd.read_csv('test_demographics.csv')
test_results = pd.read_csv('ab_test_results.csv')
test_results.date = pd.to_datetime(test_results.date)
test_results.head(n=5)

# confirming test results : to confirm that our test was administered correctly (ensure the data is sufficiently random)
# compare the size of control and variant group
test_results_grpd = test_results.groupby(by = ['group'], as_index = False)
test_results_grpd.uid.count()

# break out by the relevant demographics
test_results_demo = test_results.merge(test_demo, how = 'inner', on = 'uid')
test_results_grpd = test_results_demo.groupby(by = ['country', 'gender', 'device', 'group'], as_index = False)
test_results_grpd.uid.count()

# finding the test & control group conversion rates
test_results_grpd = test_results_demo.groupby(by = ['group'], as_index = False)
test_results_summary = test_results_grpd.agg({'purchase': ['count', 'sum']})
test_results_summary['conv'] = (test_results_summary.purchase['sum']/test_results_summary.purchase['count'])

# determine whether the difference (in conversion rate) is statistically significant? by calculating P-value
# P-value: probability under null hypothesis of obtaining a result as or more extreme than the one observed
# represents a measure of the evidence against retaining the null hypothesis

# Find the unique users in each group 
results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) 

# Find the overall number of unique users using "len" and "unique"
unique_users = len(ab_test_results.uid.unique()) 

# Find the percentage in each group
results = results / unique_users * 100
print(results)  # about 50% each group

# Find the unique users in each group, by device and gender
results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) 

# Find the overall number of unique users using "len" and "unique"
unique_users = len(ab_test_results.uid.unique())

# Find the percentage in each group
results = results / unique_users * 100
print(results) 

def get_pvalue(con_conv, test_conv, con_size, test_size):
    lift = -abs(test_conv - con_conv)
    var1 = con_conv * (1-con_conv) * (1.0/con_size)
    var2 = test_conv * (1-test_conv) * (1.0/test_size)
    sd = (var1+var2)**0.5
    p_value = 2*stats.norm.cdf(lift, loc=0, scale = sd)
    return p_value
 
# a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high 
# variance can lead to a high p-value!

## finding the test power
get_power(test_size, con_conv, test_conv, 0.95)
    
## calculating confidence intervals for our estimate
def get_ci(lift, alpha, sd):
    val=abs(stats.norm.ppf(1-alpha/2))
    lwr_bnd = lift - val*sd
    upr_bnd = lift + val*sd
    return_val = (lwr_bnd, upr_bnd)
    return return_val

lift= test_conv - con_conv
var1 = con_conv * (1-con_conv) * (1.0/con_size)
var2 = test_conv * (1-test_conv) * (1.0/test_size)
sd = (var1+var2)**0.5

get_ci(lift, 0.05, sd)

# As our standard deviation decreases so too does the width of our confidence interval. 
# our interval is very narrow thanks to our substantial lift (difference) and large sample size.

# chapter 4.2 interpeting your test results
# communicating your test results

############  what to report: use a table
#                    test group        control group
#sample size              7030               6970
#run time               2 weeks             2 weeks
#mean                    3.12                2.69
#variance                3.20                2.64
#estimated lift: 0.56*
#confidence interval: 0.56 +/- 0.4
#* significant at the 0.05 level

########### visulize result
# histogram of data, for instance you can provide a histogram of the conversion rates for each user, for test and control to compare

variant_results_rollup = test_results_rollup[test_results_rollup.group == 'V']
control_results_rollup = test_results_rollup[test_results_rollup.group == 'C']
plt.hist(variant_results_rollup['purchase'], color = 'yellow', alpha = 0.8, bins = 50, label = 'Test')       
plt.hist(control_results_rollup['purchase'], color = 'blue', alpha = 0.8, bins = 50, label = 'Control')    
plt.legend(loc='upper right')
plt.axvline(x=np.mean(variant_results_rollup['purchase']), color = 'red')
plt.axvline(x=np.mean(control_results_rollup['purchase']), color = 'green')
plt.show()

## plot the distribution using estimated means and variances

mean_control = 0.090965
mean_test = 0.102005
var_control = (mean_control * (1- mean_control))/58583
var_test = (mean_test * (1- mean_test))/56350
control_line = np.linspace(-3 * var_control**0.5 + mean_control, 3 * var_control**0.5 + mean_control, 100)
test_line = np.linspace(-3 * var_test**0.5 + mean_test, 3 * var_test**0.5 + mean_test, 100)
plt.plot(control_line, stats.norm.pdf(control_line, mean_control, var_control**0.5))
plt.plot(test_line, stats.norm.pdf(test_line, mean_test, var_test**0.5))
plt.show()

# plotting the difference of distributions
lift = mean_test - mean_control
var = var_test + var_control
diff_line = np.linspace(-3 * var**0.5 + lift, 3 * var**0.5 + lift, 100)
plt.plot(diff_line, stats.norm.pdf(diff_line, lift, var**0.5))
plt.show()

# plot the confidence interval
# coloring the regions between the bounds of the CI
# generate a set of points in this region (np.arrange())
section = np.arange(0.007624, 0.01445, 1.0/10000)
plt.fill_between(section, stats.norm.pdf(section, lift, var**0.5))
plt.plot(diff_line, stats.norm.pdf(diff_line, lift, var**0.5))
plt.show()







