

#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 3 The design and application of A/B testing

# chapter 3.1  Introduction to A/B testing

# A/B test is an experiment in which you test two different values of the same variable against one another
# randomly assign users to control and test group
# good problems for A/B testing are those where users are being impacted in an individual way
# bad problems for A/B testing are those in which there are network effects of users, dating app

demographics_data = pd.read_csv('user_demographics.csv')
demographics_data.head(4)
paywall_views = pd.read.csv('paywall_views.csv')
paywall_views.head(4)

# response variable: KPI or somthing directly related to KPI, simple to measure
# factors that impact our response, color of paywall
# experiment units: the unit over which metrics are measured before aggregating over the control or treatment group overall, users

purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data_agg = purchase_data.groupby(by = ['uid'], as_index=False)
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
total_purchase.purchase.mean()
min(total_purchase.purchase) #0
max(total_purchase.purchase) #17 varies a lot
# since the amount of time on our platform varies widely between users, so it's not meaningful to compare

# user-days as experiment unit: treat each user's action on a given day as a unique unit

purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data.date = purchase_data.date.dt.floor('d')
purchase_data_agg = purchase_data.groupby(by = ['uid', 'date'], as_index=False)
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
total_purchase.purchase.mean()
min(total_purchase.purchase) #0
max(total_purchase.purchase) #3

## Experimental units: Revenue per user day
# Extract the 'day'; value from the timestamp
purchase_data.date = purchase_data.date.dt.floor('d')

# Replace the NaN price values with 0 
purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)

# Aggregate the data by 'uid' & 'date'
purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)
revenue_user_day = purchase_data_agg.sum()

# Calculate the final average
revenue_user_day = revenue_user_day.price.mean()
print(revenue_user_day)

#########################################################################################
## chapter 3.2 preparing to run an A/B test
# there are 3 different consumable price points (price changed 3 times) which may factor into our test
# main concerns in designing a test
# 1. ensure test can be practically run 2. can derive meaningful results from it

# test sensitivity: what percentage of change would be meaningful to detect in response variable
# exercise: look at what different sensitivity look like for your experiment unit

# 1% , 10%, 20% change in revenue per user-day
revenue_user_day * 1.01 (1.1/1.2)

# it's important to understand the latent variability in the data
# standard deviation
purchase_variation = total_purchase.purchase.std()
# typically we will rely on the standard deviation of the test result in evaluating the test, use the value of the original data for planning
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
avg_purchases = total_purchase.purchase.mean()

purchase_variation / avg_purchases # small is better

#### choosing experimental unit & response variable
purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data_agg = purchase_data.groupby(by = ['uid'], as_index=False)
conversion_rate = (sum(purchase_data.purchase)/purchase_data.purchase.count())
conversion_rate ## this is the baseline

#### Conversion rate sensitivities: to examine what that value becomes under different percentage lifts and look at how many more 
# conversions per day this change would result in.

# Merge and group the datasets
purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])
purchase_data.date = purchase_data.date.dt.floor('d')

# Group and aggregate our combined dataset 
daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)
daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})

# Find the mean of each field and then multiply by 1000 to scale the result ( a sample of 0.1% of our overall population )
daily_purchases = daily_purchase_data.purchase['sum'].mean()
daily_paywall_views = daily_purchase_data.purchase['count'].mean()
daily_purchases = daily_purchases * 1000
daily_paywall_views = daily_paywall_views * 1000

print(daily_purchases)
print(daily_paywall_views)

small_sensitivity = 0.1 (0.2/0.5)

# Find the conversion rate when increased by the percentage of the sensitivity above
small_conversion_rate = conversion_rate * (1 + small_sensitivity) 

# Apply the new conversion rate to find how many more users per day that translates to
small_purchasers = daily_paywall_views * small_conversion_rate

# Subtract the initial daily_purcahsers number from this new value to see the lift
purchaser_lift = small_purchasers - daily_purchases

print(small_conversion_rate)
print(small_purchasers)
print(purchaser_lift)

############ Standard error for a conversion rate
## conversion rate p is normally distributed, SEM is std/sqrt(n) = sqrt( p*(1-p)/ n)
# Find the number of paywall views 
n = purchase_data.purchase.count()

# Calculate the quantitiy "v"
v = conversion_rate * (1 - conversion_rate) 

# Calculate the variance and standard error of the estimate
var = v / n 
se = var**0.5

print(var)
print(se)

#########################################################################################
## chapter 3.3 calculating sample sizes
# type I error: rejecting the null hypothesis when the null hypothesis is true
# type II error: retaining the false null hypothesis
# confidence level : the probability of not making a type I error (the hight cl the larger sample size needed , common 0.95)
# statistical power: the probability of finding statistically significant results when the null hypothesis is false

# power, confidence level, standard error, sensitivity and sample size
# as sample size goes up, so does power
# as confidence level goes up, power goes down

## sample size function 

from scipy import stats

def get_power(n, p1, p2, cl):
    alpha = 1-cl
    qu = stats.norm.ppf(1 - alpha/2) 
    # scipy.stats.norm.ppf compute the inverse of the CDF of the standard normal distribution
    # "percent point function" (ppf) is a terrible name. Most people in statistics just use "quantile function".
    diff = abs(p2-p1)
    se = np.sqrt((p1*(1-p1)+p2*(1-p2))/n)
    beta = stats.norm.cdf(qu*se, diff, se)
    return 1-beta
    
def get_sample_size(power, p1, p2, cl, max_n=1000000):
    n=1
    while n <= max_n:
        tmp_power = get_power(n, p1,p2, cl)
        if tmp_power >=power:
            return n
        else:
            n= n+1  # n = n+100
        
        
conversion_rate = 0.03468
sample_size_per_group = get_sample_size(0.8, conversion_rate, conversion_rate*1.1, 0.95)
print(sample_size_per_group) #45794 VS (45788)

# ppf(q, loc=0, scale=1)  # Percent point function (inverse of cdf — percentiles)  it's Z score for standard normal  
# from scipy.stats import norm
# norm.ppf(.5)  # half of the mass is before zero
# 0.0

#The equivalent of the R pnorm() function is: scipy.stats.norm.cdf() with python 
#The equivalent of the R qnorm() function is: scipy.stats.norm.ppf() with python

#The function pnorm returns the integral from −∞ to q of the pdf of the normal distribution where q is a Z-score. 
#Try to guess the value of pnorm(0). (pnorm has the same default mean and sd arguments as dnorm).
#pnorm(0) # 0.5


###### ways to decrease the needed sample size
# 1. switch the unit of observation in a way that reduces variability in the data,for instance switch from revenue to conversion rate
# 2. exclude users who are irrelevant to the process

#decreasing our confidence level has a slightly larger impact on the power than increasing our sample size

# Merge the demographics and purchase data to only include paywall views
purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])
                            
# Find the conversion rate
conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())

# Desired Power: 0.95
# CL 0.90
# Percent Lift: 0.1
p2 = conversion_rate * (1 + 0.1)
sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)
print(sample_size)



#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 4 analyzing A/B testing results

# chapter 4.1  analyzing the A/B test results

# test results
test_demographics = pd.read_csv('test_demographics.csv')
test_results = pd.read_csv('ab_test_results.csv')
test_results.date = pd.to_datetime(test_results.date)
test_results.head(n=5)

# confirming test results : to confirm that our test was administered correctly (ensure the data is sufficiently random)
# compare the size of control and variant group
test_results_grpd = test_results.groupby(by = ['group'], as_index = False)
test_results_grpd.uid.count()

# break out by the relevant demographics
test_results_demo = test_results.merge(test_demo, how = 'inner', on = 'uid')
test_results_grpd = test_results_demo.groupby(by = ['country', 'gender', 'device', 'group'], as_index = False)
test_results_grpd.uid.count()

# finding the test & control group conversion rates
test_results_grpd = test_results_demo.groupby(by = ['group'], as_index = False)
test_results_summary = test_results_grpd.agg({'purchase': ['count', 'sum']})
test_results_summary['conv'] = (test_results_summary.purchase['sum']/test_results_summary.purchase['count'])

# determine whether the difference (in conversion rate) is statistically significant? by calculating P-value
# P-value: probability under null hypothesis of obtaining a result as or more extreme than the one observed
# represents a measure of the evidence against retaining the null hypothesis

# Find the unique users in each group 
results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) 

# Find the overall number of unique users using "len" and "unique"
unique_users = len(ab_test_results.uid.unique()) 

# Find the percentage in each group
results = results / unique_users * 100
print(results)  # about 50% each group

# Find the unique users in each group, by device and gender
results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) 

# Find the overall number of unique users using "len" and "unique"
unique_users = len(ab_test_results.uid.unique())

# Find the percentage in each group
results = results / unique_users * 100
print(results) 

def get_pvalue(con_conv, test_conv, con_size, test_size):
    lift = -abs(test_conv - con_conv)
    var1 = con_conv * (1-con_conv) * (1.0/con_size)
    var2 = test_conv * (1-test_conv) * (1.0/test_size)
    sd = (var1+var2)**0.5
    p_value = 2*stats.norm.cdf(lift, loc=0, scale = sd)
    return p_value
 
# a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high 
# variance can lead to a high p-value!

## finding the test power
get_power(test_size, con_conv, test_conv, 0.95)
    
## calculating confidence intervals for our estimate
def get_ci(lift, alpha, sd):
    val=abs(stats.norm.ppf(1-alpha/2))
    lwr_bnd = lift - val*sd
    upr_bnd = lift + val*sd
    return_val = (lwr_bnd, upr_bnd)
    return return_val

lift= test_conv - con_conv
var1 = con_conv * (1-con_conv) * (1.0/con_size)
var2 = test_conv * (1-test_conv) * (1.0/test_size)
sd = (var1+var2)**0.5

get_ci(lift, 0.05, sd)

# As our standard deviation decreases so too does the width of our confidence interval. 
# our interval is very narrow thanks to our substantial lift (difference) and large sample size.

# chapter 4.2 interpeting your test results
# communicating your test results

############  what to report: use a table
#                    test group        control group
#sample size              7030               6970
#run time               2 weeks             2 weeks
#mean                    3.12                2.69
#variance                3.20                2.64
#estimated lift: 0.56*
#confidence interval: 0.56 +/- 0.4
#* significant at the 0.05 level

########### visulize result
# histogram of data, for instance you can provide a histogram of the conversion rates for each user, for test and control to compare

variant_results_rollup = test_results_rollup[test_results_rollup.group == 'V']
control_results_rollup = test_results_rollup[test_results_rollup.group == 'C']
plt.hist(variant_results_rollup['purchase'], color = 'yellow', alpha = 0.8, bins = 50, label = 'Test')       
plt.hist(control_results_rollup['purchase'], color = 'blue', alpha = 0.8, bins = 50, label = 'Control')    
plt.legend(loc='upper right')
plt.axvline(x=np.mean(variant_results_rollup['purchase']), color = 'red')
plt.axvline(x=np.mean(control_results_rollup['purchase']), color = 'green')
plt.show()

## plot the distribution using estimated means and variances

mean_control = 0.090965
mean_test = 0.102005
var_control = (mean_control * (1- mean_control))/58583
var_test = (mean_test * (1- mean_test))/56350
control_line = np.linspace(-3 * var_control**0.5 + mean_control, 3 * var_control**0.5 + mean_control, 100)
test_line = np.linspace(-3 * var_test**0.5 + mean_test, 3 * var_test**0.5 + mean_test, 100)
plt.plot(control_line, stats.norm.pdf(control_line, mean_control, var_control**0.5))
plt.plot(test_line, stats.norm.pdf(test_line, mean_test, var_test**0.5))
plt.show()

# plotting the difference of distributions
lift = mean_test - mean_control
var = var_test + var_control
diff_line = np.linspace(-3 * var**0.5 + lift, 3 * var**0.5 + lift, 100)
plt.plot(diff_line, stats.norm.pdf(diff_line, lift, var**0.5))
plt.show()

# plot the confidence interval
# coloring the regions between the bounds of the CI
# generate a set of points in this region (np.arrange())
section = np.arange(0.007624, 0.01445, 1.0/10000)
plt.fill_between(section, stats.norm.pdf(section, lift, var**0.5))
plt.plot(diff_line, stats.norm.pdf(diff_line, lift, var**0.5))
plt.show()







