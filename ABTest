
#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 1 key performace indicators (KPI): measuring business success

# A/B testing: test two or more different ideas against each other, see which one empirically performs better
# running the test in real world, no guessing
# provide accurate answers quickly
# allows companies to rapidly iterate on ideas
# a statistically sound way to establish causal relationships (Randomness helps ensure nothing else is impacting our observed results.)

# where can A/B test be used? users + ideas --> A/B test
# test impact of drugs
# mobile games to incentivize user spending
# subscription services driving user growth

# chapter 1.1 KPI (run A/B test to improve KPI)
# drug remission rate, likelihood of drug side effect, revenue, conversion rate, game play time per user
# identify KPI: experience + domain knowledge + exploratory data analysis
# experience + domain knowledge: what's likely to be an important driver of the business
# exploratory data analysis: uncover relationships that reveal which metrics truly measure these drivers

# example: medication app
# services: 1. paid subscription 2. one-off in-app purchase
# goals/KPI: maintain high conversion rate
# additionally we want to maintain strength in a variety of other business areas as well

import pandas as pd
# load customer demographics
customer_demographics = pd.read_csv('customer_demographics.csv')
print(customer_demographics.head())

# load customer subscriptions (user actions)
customer_subscriptions = pd.read_csv('customer_subscriptions.csv')
print(customer_subscriptions.head())

## focus on the KPI of conversion rate (will consider a variety of others)
# one question in defining KPI: over what interval should we consider the conversion rate?
# one way to decide is to see the generalizability of these statistics across different demographic groups
#  * stable, generalizable KPIs are better than custom KPIs. stability is desired so we don't need custom KPIs for each breakdown
# second is to see if one is more correlated with important factors like retention or spending than the others
# pd.merge(df1, df2) or df1.merge(df2)
sub_data_demo = customer_demographics.merge(customer_subscriptions, how = 'inner', on = ['uid'])  ## inner, outer, left, right
sub_data_demo.head()
# aggregate combined dataset to calculate potential KPIs

## exercises : Loading & examining our data
# user demographics and a set of data relating to in-app purchases for our meditation app.

# Import pandas 
import pandas as pd

# Load the customer_data
customer_data = pd.read_csv('customer_data.csv')
# Load the app_purchases
app_purchases = pd.read_csv('inapp_purchases.csv')

# Print the columns of customer data
print(customer_data.columns)
# Print the columns of app_purchases
print(app_purchases.columns)

# Merge on the 'uid' and 'date' field
uid_date_combined_data = app_purchases.merge(customer_data, on=['uid', 'date'], how='inner')

# Examine the results 
print(uid_date_combined_data.head())
print(len(uid_date_combined_data))


######## chapter 1.2 exploratory analysis if KPIs
# which conversion rate metric is the most appropriate
# most company will have many KPIs, each serving a different purpose. conversion rate is just one KPI
# calculating KPI and measure performance across different groups
# DataFrame.groupby(by =None, axis=0, level=None, as_index=True, sort=True, group_keys=True, squeeze=False, **kwargs)
# DataFrame.agg(func, axis =0, *args, **kwargs)

sub_data_grp = sub_data_demo.groupby(by = ['country', 'device'], axis=0, as_index=False)
sub_data_grp.price.mean()
sub_data_grp.price.agg('mean')
sub_data_grp.price.agg(['mean', 'median'])
sub_data_grp.agg({'price': ['mean', 'min', 'max'], 'age': ['mean', 'min', 'max']})

# custom functions, removes the top and bottom ten percent of values before calculating the mean
def truncated_mean(data):
    top_val = data.quantile(0.9)
    bot_val = data.quantile(0.1)
    trunc_data = data[(data <= top_val) & (data >= bot_val)]
    mean = trunc_data.mean()
    return(mean)
    
sub_data_grp.agg({'age':[truncated_mean]})    # no quotation marks for custom functions

# exploring the in-app purchase data in more detail.
# examine the results to get an understanding of the overall data

# Calculate the mean and median of price and age
purchase_summary = purchase_data.agg({'price': ['mean', 'median'], 'age': ['mean', 'median']})
# Examine the output 
# mean is higher than the median? This suggests that we have some users who are making a lot of purchases!
print(purchase_summary)

# calculate a set of summary statistics about the purchase data broken out by 'device' (Android or iOS) and 'gender' (Male or Female).
# Following this, you'll compare the values across these subsets, which will give you a baseline for these values as potential 
# KPIs to optimize going forward.

# These values provide a great summary of the customer data which will be useful as you move to optimizing the conversion rate.
# Group the data 
grouped_purchase_data = purchase_data.groupby(by = ['device', 'gender'])

# Aggregate the data
purchase_summary = grouped_purchase_data.agg({'price': ['mean', 'median', 'std']})

# Examine the results
print(purchase_summary)


######## chapter 1.3 calculating KPIs - a practical example
# comparing KPIs "user conversion rate" after free trial ends
# 1. first week after the trial ends

import numpy as np
from datetime import datetime, timedelta
current_date = pd.to_datetime('2018-03-17')
# "lapse date" = date that the trial ended
print(sub_data_demo.lapse_date.max())  #'2018-03-17'

# remove users whose lapse date with 7 days
max_lapse_date = current_date - timedelta(days=7)
# restrict to users lapsed before max_lapse_date
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]
# count the users
total_users_count = conv_sub_data.price.count()
print(total_users_count)

# latest subscription date: within 7 days of lapsing
max_sub_date = conv_sub_data.lapse_date + timedelta(days = 7)

# filter the users with non-zero subscription price and who subscribed before max_sub_date
total_subs = conv_sub_data[(conv_sub_data.price > 0) & (conv_sub_data.subscription_date <= max_sub_date)]

# count the users
total_subs_count = total_subs.price.count()
print(total_subs_count)

conversion_rate = total_subs_count/total_users_count
print(conversion_rate) #0.2325

### check week one and week two conversion rates across different cohorts
max_lapse_date = current_date - timedelta(days=14)
# restrict to users lapsed before max_lapse_date
#conv_sub_data_7days = conv_sub_data.copy()
conv_sub_data = sub_data_demo[(sub_data_demo.lapse_date < max_lapse_date)]

# find number of days between subscription and lapse date if subscribed
# ... and pd.NaT otherwise
sub_time = np.where(conv_sub_data.subscription_date.notnull(), (conv_sub_data.subscription_date - \
           conv_sub_data.lapse_date).dt.days, pd.NaT)
conv_sub_data['sub_time'] = sub_time

# gcr7(), gcr14: conversion rates over 7 and 14 days
def gcr7(sub_time):
    return sum(sub_time <= 7) / sub_time.count()

# form cohorts

    
    














#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 3 The design and application of A/B testing

# chapter 3.1  Introduction to A/B testing

# A/B test is an experiment in which you test two different values of the same variable against one another
# randomly assign users to control and test group
# good problems for A/B testing are those where users are being impacted in an individual way
# bad problems for A/B testing are those in which there are network effects of users, dating app

demographics_data = pd.read_csv('user_demographics.csv')
demographics_data.head(4)
paywall_views = pd.read.csv('paywall_views.csv')
paywall_views.head(4)

# response variable: KPI or somthing directly related to KPI, simple to measure
# factors that impact our response, color of paywall
# experiment units: the unit over which metrics are measured before aggregating over the control or treatment group overall, users

purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data_agg = purchase_data.groupby(by = ['uid'], as_index=False)
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
total_purchase.purchase.mean()
min(total_purchase.purchase) #0
max(total_purchase.purchase) #17 varies a lot
# since the amount of time on our platform varies widely between users, so it's not meaningful to compare

# user-days as experiment unit: treat each user's action on a given day as a unique unit

purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data.date = purchase_data.date.dt.floor('d')
purchase_data_agg = purchase_data.groupby(by = ['uid', 'date'], as_index=False)
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
total_purchase.purchase.mean()
min(total_purchase.purchase) #0
max(total_purchase.purchase) #3

## Experimental units: Revenue per user day
# Extract the 'day'; value from the timestamp
purchase_data.date = purchase_data.date.dt.floor('d')

# Replace the NaN price values with 0 
purchase_data.price = np.where(np.isnan(purchase_data.price), 0, purchase_data.price)

# Aggregate the data by 'uid' & 'date'
purchase_data_agg = purchase_data.groupby(by=['uid', 'date'], as_index=False)
revenue_user_day = purchase_data_agg.sum()

# Calculate the final average
revenue_user_day = revenue_user_day.price.mean()
print(revenue_user_day)

#########################################################################################
## chapter 3.2 preparing to run an A/B test
# there are 3 different consumable price points (price changed 3 times) which may factor into our test
# main concerns in designing a test
# 1. ensure test can be practically run 2. can derive meaningful results from it

# test sensitivity: what percentage of change would be meaningful to detect in response variable
# exercise: look at what different sensitivity look like for your experiment unit

# 1% , 10%, 20% change in revenue per user-day
revenue_user_day * 1.01 (1.1/1.2)

# it's important to understand the latent variability in the data
# standard deviation
purchase_variation = total_purchase.purchase.std()
# typically we will rely on the standard deviation of the test result in evaluating the test, use the value of the original data for planning
total_purchases = purchase_data_agg.purchase.sum()   #purchase field 0/1 value
total_purchases.purchase = np.where(np.isnan(total_purchase.purchase), 0, total_purchase.purchase)
avg_purchases = total_purchase.purchase.mean()

purchase_variation / avg_purchases # small is better

#### choosing experimental unit & response variable
purchase_data = demogrphics_data.merge(paywall_views, how = 'left', on = ['uid'])
purchase_data_agg = purchase_data.groupby(by = ['uid'], as_index=False)
conversion_rate = (sum(purchase_data.purchase)/purchase_data.purchase.count())
conversion_rate ## this is the baseline

#### Conversion rate sensitivities: to examine what that value becomes under different percentage lifts and look at how many more 
# conversions per day this change would result in.

# Merge and group the datasets
purchase_data = demographics_data.merge(paywall_views,  how='inner', on=['uid'])
purchase_data.date = purchase_data.date.dt.floor('d')

# Group and aggregate our combined dataset 
daily_purchase_data = purchase_data.groupby(by=['date'], as_index=False)
daily_purchase_data = daily_purchase_data.agg({'purchase': ['sum', 'count']})

# Find the mean of each field and then multiply by 1000 to scale the result ( a sample of 0.1% of our overall population )
daily_purchases = daily_purchase_data.purchase['sum'].mean()
daily_paywall_views = daily_purchase_data.purchase['count'].mean()
daily_purchases = daily_purchases * 1000
daily_paywall_views = daily_paywall_views * 1000

print(daily_purchases)
print(daily_paywall_views)

small_sensitivity = 0.1 (0.2/0.5)

# Find the conversion rate when increased by the percentage of the sensitivity above
small_conversion_rate = conversion_rate * (1 + small_sensitivity) 

# Apply the new conversion rate to find how many more users per day that translates to
small_purchasers = daily_paywall_views * small_conversion_rate

# Subtract the initial daily_purcahsers number from this new value to see the lift
purchaser_lift = small_purchasers - daily_purchases

print(small_conversion_rate)
print(small_purchasers)
print(purchaser_lift)

############ Standard error for a conversion rate
## conversion rate p is normally distributed, SEM is std/sqrt(n) = sqrt( p*(1-p)/ n)
# Find the number of paywall views 
n = purchase_data.purchase.count()

# Calculate the quantitiy "v"
v = conversion_rate * (1 - conversion_rate) 

# Calculate the variance and standard error of the estimate
var = v / n 
se = var**0.5

print(var)
print(se)

#########################################################################################
## chapter 3.3 calculating sample sizes
# type I error: rejecting the null hypothesis when the null hypothesis is true
# type II error: retaining the false null hypothesis
# confidence level : the probability of not making a type I error (the hight cl the larger sample size needed , common 0.95)
# statistical power: the probability of finding statistically significant results when the null hypothesis is false

# power, confidence level, standard error, sensitivity and sample size
# as sample size goes up, so does power
# as confidence level goes up, power goes down

## sample size function 

from scipy import stats

def get_power(n, p1, p2, cl):
    alpha = 1-cl
    qu = stats.norm.ppf(1 - alpha/2) 
    # scipy.stats.norm.ppf compute the inverse of the CDF of the standard normal distribution
    # "percent point function" (ppf) is a terrible name. Most people in statistics just use "quantile function".
    diff = abs(p2-p1)
    se = np.sqrt((p1*(1-p1)+p2*(1-p2))/n)
    beta = stats.norm.cdf(qu*se, diff, se)
    return 1-beta
    
def get_sample_size(power, p1, p2, cl, max_n=1000000):
    n=1
    while n <= max_n:
        tmp_power = get_power(n, p1,p2, cl)
        if tmp_power >=power:
            return n
        else:
            n= n+1  # n = n+100
        
        
conversion_rate = 0.03468
sample_size_per_group = get_sample_size(0.8, conversion_rate, conversion_rate*1.1, 0.95)
print(sample_size_per_group) #45794 VS (45788)

# ppf(q, loc=0, scale=1)  # Percent point function (inverse of cdf — percentiles)  it's Z score for standard normal  
# from scipy.stats import norm
# norm.ppf(.5)  # half of the mass is before zero
# 0.0

#The equivalent of the R pnorm() function is: scipy.stats.norm.cdf() with python 
#The equivalent of the R qnorm() function is: scipy.stats.norm.ppf() with python

#The function pnorm returns the integral from −∞ to q of the pdf of the normal distribution where q is a Z-score. 
#Try to guess the value of pnorm(0). (pnorm has the same default mean and sd arguments as dnorm).
#pnorm(0) # 0.5


###### ways to decrease the needed sample size
# 1. switch the unit of observation in a way that reduces variability in the data,for instance switch from revenue to conversion rate
# 2. exclude users who are irrelevant to the process

#decreasing our confidence level has a slightly larger impact on the power than increasing our sample size

# Merge the demographics and purchase data to only include paywall views
purchase_data = demographics_data.merge(paywall_views, how='inner', on=['uid'])
                            
# Find the conversion rate
conversion_rate = (sum(purchase_data.purchase) / purchase_data.purchase.count())

# Desired Power: 0.95
# CL 0.90
# Percent Lift: 0.1
p2 = conversion_rate * (1 + 0.1)
sample_size = get_sample_size(0.95, conversion_rate, p2, 0.90)
print(sample_size)



#$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

# chapter 4 analyzing A/B testing results

# chapter 4.1  analyzing the A/B test results

# test results
test_demographics = pd.read_csv('test_demographics.csv')
test_results = pd.read_csv('ab_test_results.csv')
test_results.date = pd.to_datetime(test_results.date)
test_results.head(n=5)

# confirming test results : to confirm that our test was administered correctly (ensure the data is sufficiently random)
# compare the size of control and variant group
test_results_grpd = test_results.groupby(by = ['group'], as_index = False)
test_results_grpd.uid.count()

# break out by the relevant demographics
test_results_demo = test_results.merge(test_demo, how = 'inner', on = 'uid')
test_results_grpd = test_results_demo.groupby(by = ['country', 'gender', 'device', 'group'], as_index = False)
test_results_grpd.uid.count()

# finding the test & control group conversion rates
test_results_grpd = test_results_demo.groupby(by = ['group'], as_index = False)
test_results_summary = test_results_grpd.agg({'purchase': ['count', 'sum']})
test_results_summary['conv'] = (test_results_summary.purchase['sum']/test_results_summary.purchase['count'])

# determine whether the difference (in conversion rate) is statistically significant? by calculating P-value
# P-value: probability under null hypothesis of obtaining a result as or more extreme than the one observed
# represents a measure of the evidence against retaining the null hypothesis

# Find the unique users in each group 
results = ab_test_results.groupby('group').agg({'uid': pd.Series.nunique}) 

# Find the overall number of unique users using "len" and "unique"
unique_users = len(ab_test_results.uid.unique()) 

# Find the percentage in each group
results = results / unique_users * 100
print(results)  # about 50% each group

# Find the unique users in each group, by device and gender
results = ab_test_results.groupby(by=['group', 'device', 'gender']).agg({'uid': pd.Series.nunique}) 

# Find the overall number of unique users using "len" and "unique"
unique_users = len(ab_test_results.uid.unique())

# Find the percentage in each group
results = results / unique_users * 100
print(results) 

def get_pvalue(con_conv, test_conv, con_size, test_size):
    lift = -abs(test_conv - con_conv)
    var1 = con_conv * (1-con_conv) * (1.0/con_size)
    var2 = test_conv * (1-test_conv) * (1.0/test_size)
    sd = (var1+var2)**0.5
    p_value = 2*stats.norm.cdf(lift, loc=0, scale = sd)
    return p_value
 
# a large lift makes us confident in our observed result, while a small sample size makes us less so, and ultimately high 
# variance can lead to a high p-value!

## finding the test power
get_power(test_size, con_conv, test_conv, 0.95)
    
## calculating confidence intervals for our estimate
def get_ci(lift, alpha, sd):
    val=abs(stats.norm.ppf(1-alpha/2))
    lwr_bnd = lift - val*sd
    upr_bnd = lift + val*sd
    return_val = (lwr_bnd, upr_bnd)
    return return_val

lift= test_conv - con_conv
var1 = con_conv * (1-con_conv) * (1.0/con_size)
var2 = test_conv * (1-test_conv) * (1.0/test_size)
sd = (var1+var2)**0.5

get_ci(lift, 0.05, sd)

# As our standard deviation decreases so too does the width of our confidence interval. 
# our interval is very narrow thanks to our substantial lift (difference) and large sample size.

# chapter 4.2 interpeting your test results
# communicating your test results

############  what to report: use a table
#                    test group        control group
#sample size              7030               6970
#run time               2 weeks             2 weeks
#mean                    3.12                2.69
#variance                3.20                2.64
#estimated lift: 0.56*
#confidence interval: 0.56 +/- 0.4
#* significant at the 0.05 level

########### visulize result
# histogram of data, for instance you can provide a histogram of the conversion rates for each user, for test and control to compare

variant_results_rollup = test_results_rollup[test_results_rollup.group == 'V']
control_results_rollup = test_results_rollup[test_results_rollup.group == 'C']
plt.hist(variant_results_rollup['purchase'], color = 'yellow', alpha = 0.8, bins = 50, label = 'Test')       
plt.hist(control_results_rollup['purchase'], color = 'blue', alpha = 0.8, bins = 50, label = 'Control')    
plt.legend(loc='upper right')
plt.axvline(x=np.mean(variant_results_rollup['purchase']), color = 'red')
plt.axvline(x=np.mean(control_results_rollup['purchase']), color = 'green')
plt.show()

## plot the distribution using estimated means and variances

mean_control = 0.090965
mean_test = 0.102005
var_control = (mean_control * (1- mean_control))/58583
var_test = (mean_test * (1- mean_test))/56350
control_line = np.linspace(-3 * var_control**0.5 + mean_control, 3 * var_control**0.5 + mean_control, 100)
test_line = np.linspace(-3 * var_test**0.5 + mean_test, 3 * var_test**0.5 + mean_test, 100)
plt.plot(control_line, stats.norm.pdf(control_line, mean_control, var_control**0.5))
plt.plot(test_line, stats.norm.pdf(test_line, mean_test, var_test**0.5))
plt.show()

# plotting the difference of distributions
lift = mean_test - mean_control
var = var_test + var_control
diff_line = np.linspace(-3 * var**0.5 + lift, 3 * var**0.5 + lift, 100)
plt.plot(diff_line, stats.norm.pdf(diff_line, lift, var**0.5))
plt.show()

# plot the confidence interval
# coloring the regions between the bounds of the CI
# generate a set of points in this region (np.arrange())
section = np.arange(0.007624, 0.01445, 1.0/10000)
plt.fill_between(section, stats.norm.pdf(section, lift, var**0.5))
plt.plot(diff_line, stats.norm.pdf(diff_line, lift, var**0.5))
plt.show()







