# step 1: exploratory data analysis--organizing, ploting, computing a few numerical summaries and draw conclusion from data (John Tukey)
# importing data 
import pandas as pd
df_swing = pd.read_csv('2018_swing_states.csv')
df_swing[['state', 'county', 'dem_share']]

####  Chapter 1:  graphical exploratory data analysis: histogram, bee swarm plot, ECDF
####  Chapter 1.1 histogram
## use the default settings of Seaborn: a matplotlib based statistical data visualization package
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

# hostogram: the height of each bar is the number of (y) that had the give level (x)

_ =  plt.hist(df_swing['dem_share']) # pass a column in a dataframe, or a NumPy array
_ = plt.xlabel('percent of vaote for Obama') # always label axes
_ = plt.ylabel('number of counties')
plt.show()

# histogram have binning bias: the same data maybe interpreted differently depending on choice of bins
# not all data is plotted, data are sweeped into bins and loosing their actual values
bin_edges = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
_ =  plt.hist(df_swing['dem_share'], bins = bin_edges) # specifiy bin edges

# A commonly used rule of thumb for choosing number of bins: the "Square root rule"
import numpy as np
#compute number of data points
n_data = len(df_swing['dem_share'])
n_bins = int(np.sqrt(n_data))
_ =  plt.hist(df_swing['dem_share'], bins = n_bins) # or specify the number of bins 

####  Chapter 1.2 bee swarm plot
# plot all of your data: bee swarm plots
_ = sns.swarmplot(x='state', y='dem_share', data = df_swing)
_ = plt.xlabel('state')
_ = plt.ylabel('percent of vote for Obama')
plt.show()

# bee swarm plots have a limit to their efficacy

####  Chapter 1.3 Empirical cumulative distribution function (ECDF) 
In a workflow, it's almost always plot the ECDF first, it shows all the data and give a complete picture of how the data are distributed
# a X-value of ECDF is the quantity you are measuring
# the y_value is the fraction of data points that have a value smaller than the corresponding x-value

imporot numpy as np
### the ecdf function takes a 1D array of data as input and returns the x and y values of the ECDF
def ecdf(data):
    n = len(data)
    x = np.sort(data)
    y = np.arange(1, n+1) /n
    return x, y
    
x , y = ecdf(df_swing['dem_share'])
_ = plt.plot(x, y, marker = '.', linestyle = 'none')
_ = plt.xlabel('percent of vote for Obama')
_ =  plt.ylabel('ECDF')
plt.margins(0.02) # keep data off plot edges
plt.show()

# compare 2 or more distributions using ECDFs
x_set, y_set = ecdf(setosa_petal_length)
x_vers, y_vers = ecdf(versicolor_petal_length)
x_virg, y_virg = ecdf(virginica_petal_length)

# Plot all ECDFs on the same plot
_ = plt.plot(x_set, y_set, marker ='.', linestyle = 'none')
_ = plt.plot(x_vers, y_vers, marker ='.', linestyle = 'none')
_ = plt.plot(x_virg, y_virg, marker ='.', linestyle = 'none')

# Annotate the plot
plt.legend(('setosa', 'versicolor', 'virginica'), loc='lower right')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# Display the plot
plt.show()

### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 2: Quantitative exploratory data analysis: summary statistics 

#### Chapter 2.1 sample mean and median
# mean: major problem is that it is heavily influenced by outliers
# outliers are data points whose value is far greater or less than most of the rest of the data
import numpy as np
np.mean(dem_share_PA)

# median: the middle value of a data set, a summary statistic that is immune to extreme data
np.median(dem_share_UT)

#### Chapter 2.2 percentiles, outliers and box plots
# computing percentiles
np.percentile(df_swing['dem_share'], [25, 50, 75])

# box plot: The center of the box is the median. 
#           The edges of the boxes are the 25th and 75th percentile. 
#           The total height of the box contains the middle 50% of the data, and is called the interquartile range, or IQR. 
#           The whiskers extend a distance of 1.5 times the IQR, or to the extent of the data, whichever is more extreme.
#           All points outside of the whishers are plotted as individual poitns,, which we often demarcates as outliers

## there is no single defination for an outlier, being more than 2 IQRs away from the median is a common criterion. It's not necessarily erroneous
import matplotlib.pyplot as plt
import seaborn as sns
_ = sns.boxplot(x='east_west', y = 'dem_share', data = df_all_states)
_ = plt.xlabel('region')
_ = plt.ylabel('percent of vote for Obama')
plt.show()

## exercise
percentiles = np.array([2.5, 25, 50, 75, 97.5])
ptiles = np.percentile(versicolor_petal_length, percentiles)
print(ptiles)

# compare percentiles to ECDF
# plot the ECDF
x_vers, y_vers = ecdf(versicolor_petal_length)
_ = plt.plot(x_vers, y_vers, marker = '.', linestyle = 'none')
_ = plt.xlabel('petal length (cm)')
_ = plt.ylabel('ECDF')

# overlay percentiles as red diamnds
_ = plt.plot(ptiles, percentiles/100, marker='D', color = 'red', linestyle = 'none')
plt.show()


#### Chapter 2.3 variance and standard deviation
# variance: the mean squared distance of the data from their mean. it's a meansure of the spread of the data
np.var(dem_share_FL)
# standard deviation: squre root of the variance
np.std(dem_share_FL)
np.sqrt(np.var(dem_share_FL))

# manually computing variance
differences = versicolor_petal_length - np.mean(versicolor_petal_length)
diff_sq = difference**2
variance_explicit = np.mean(diff_sq)
variance_np = np.var(versicolor_petal_length)
print(variance_explicit, variance_np)

#### Chapter 2.4 covariance and pearson correlation coefficient

# to explore the relations between variables, you can draw scatter plot 
_ = plt.plot(total_votes/1000, dem_share, maker = '.', linestyle = 'none')
_ = plt.xlabel('total votes (thousands)')
_ = plt.ylabel('percent of vote for Obama')

# covariance: the mean of the product of the values differences from the mean of each variable, 
# it's a measure of how two quantities vary togather
# standardize covariance: Pearson correlation coefficient = covariance / standard deviations of each variable 
# it's the comparision of the variability in the data due to codependence (covariance) to the variability inherent to each variable indepently (their standard deviations)
# it's dimentionless, ranges from -1 (complete unti correlation) to 1 (complete correlation)

# computing the covariance
# computing the covariance matrix 
covariance_matrix = np.cov(versicolor_petal_length, versicolor_petal_width)
print(covariance_matrix)
petal_cov = covariance_matrix[0, 1] #  covariance_matrix[1, 0] get the same result
print(petal_cov)

## computing the Pearson correlation coefficient
def pearson_r(x, y):
    """ 
    Compute Pearson correlation coefficient between two arrays
    """
    corr_mat = np.corrcoef(x,y)
    return corr_mat[0, 1]
    
r = pearson_r(versicolor_petal_length, versicolor_petal_width)
print(r)


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 3: thinking probabilistically -- discrete variables

#### Chapter 3.1 probabilistic logic and statistical inference
# probabilistic reasoning allows us to describe uncertainty
# statistical inference: given a set of data, you draw probabilistically conclusions about what you might expect if those data were 
# acquired again and again and again

#### Chapter 3.2 random number generators and hacker statistics
# hacker statistics: instead of literally repeating the data acquisition again and again, we simulate repeated measurements to compute probabilities
# np.random module in Python is the main engin of hacker statistics

# simulate the outcom of successive 4 coin flips, what's the probability of getting 4 heads
# Bernoulli trial: an experiment that has two outcoms, success (True) and failure (False)
# np.random.ramdom(): draw a number between 0 and 1 such that all numbers in this interval are equally likely to occur
import numpy as np
np.random.seed(42) # to make the generated random data reproducible
random_numbers = np.random.ramdom(size = 4)
print(random_numbers)
heads = random_numbers < 0.5
np.sum(heads)

n_all_heads = 0
for _ in range(10000):
    heads = np.random.random(size = 4) < 0.5
    n_heads = np.sum(heads)
    if n_heads == 4:
        n_all_heads +=1
p_all_heads = n_all_heads /10000

# the distribution of data generated by np.random.random
np.random.seed(42)
random_numbers = np.empty(100000)
for i in range(100000):
    random_numbers[i] = np.random.random()
_ = plt.hist(random_numbers)
plt.show()

## simulate Bernoulli trials
def perform_bernoulli_trials(n, p):
    """Perform n Bernoulli trials with success probability p and return number of success."""
    n_success = 0
    for i in range(n):
        random_number = np.random.random()
        if random number < p:
            n_success +=1
            
   # return sum(np.random.random(size = n) < p)
   return n_success
    
# given the probability of a default is p = 0.05, how many defaults might we expect
np.ramdom.seed(42)
n_defaults = np.empty(1000)
for i in range(1000):
    n_defaults[i] = perform_bernoulli_trials(100, 0.05)

_ = plt.hist(n_defaults, normed = True)
_ = plt.xlabel('number of experiment')
_ = plt.ylabel('probability of default')
plt.show()

# will the bank fail? 
# If interest rates are such that the bank will lose money if 10 or more of its loans are defaulted upon, what is the probability 
# that the bank will lose money?
x,y = ecdf(n_defaults)
_ = plt.plot(x,y, marker = '.', linestyle = 'none')
_ = plt.xlabel('number of defaults out of 100')
_ = plt.ylabel('CDF')
plt.show()

# compute the number of 100-loan simulations with 10 or more defaults
n_lose_money = np.sum(n_defaults >= 10)
# compute and print the probability of losing money
print('Probability of losing money = ', n_lose_money/ len(n_defaults))

#### Chapter 3.3 probability distributions and stories: the Binomial distribution
# probability mass function (PMF): the set of probabilities of discrete outcomes
# distribution: a mathematical description of outcomes
# match a story to a mathematical description of probabilities
# the number r of successes in n bernoulli trials with probability p of success is Binomially distributed

# a more efficient way to simulate Bernoulli trials
# take 10000 samples out of the Bernoulli distribution: n_defaults
n_defaults = np.random.binomial(n=100, p=0.05, size = 10000)
x,y = ecdf(n_defaults)
_ = plt.plot(x,y, marker = '.', linestyle='none')
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('CDF')
plt.show()

# plotting the Binomial PMF, the trick is setting up the edges of the bins to pass to plt.hist() via bins keyword
# we want the bins centered on the integers, so the edges of the bins should be -0.5, 0.5, 1.5, 2.5, ... up to max(n_defaults) + 1.5
bins = np.arange(0, np.max(n_defaults) + 1.5) - 0.5
_ = plt.hist(n_defaults, normed = True, bins = bins)
_ = plt.xlabel('number of defaults out of 100 loans')
_ = plt.ylabel('PMF')
plt.show()

#### Chapter 3.4 Poisson processes and the Poisson distribution
# Poisson process: the timing of the next event is completely independent of when the previous event happened
# the number of arrivals of a Poisson process in a given amount of time is Poisson distributed
# Poisson distribution has one parameter: the average number of arrivals in a given length of time
# Poisson distribution is a limit of the Binomial distribution for low probability of success and large number of trials, i.e. for rare events
# examples: 1.natural birth in a given hospital are a Poisson process 2. hits on a website

# the Poisson distribution with arrival rate equal to np approximates a Binomial distribution for n Bernoulli trials with 
# probability p of success (with n large and p small)
# Poisson distribution with an arrival rate of 10, Binomial distribution with parameters n and p such that np=10
samples_poisson = np.random.poisson(10, size = 10000)
print('Poisson:  ', np.mean(samples_poisson), np.std(samples_poisson))

# specifiy values of n and p to consider for Binomial: n, p 
n = [20, 100, 1000]
p = [0.5, 0.1, 0.01]
for i in range(3):
    samples_binomial = np.random.binomial(n=n[i], p = p[i], size = 10000)
    print('n = ', n[i], 'Binom: ', np.mean(samples_binomial), np.std(samples_binomial))

# the mean is 251/115, what's the probability of having 7 or more in a season
n_nohitters = np.random.poisson(251/115, size = 10000)
n_large = np.sum(n_nohitters >= 7)
p_large = n_large/10000
print('Probability of seven or more no-hitters:', p_large)


### $$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$

#### Chapter 4: thinking probabilistically -- continuous variables
# probability density function (PDF):  mathematical description of the relative likelihood of observing a value of a continuous variable
# the probability is given by the area under the PDF

#goal: computing parameters and their confidence intervals, and do hypothesis tests

#### Chapter 4.1 normal distribution 
# normal distribution: it describes a continuous variable whose PDF is symmetric and has a single peak
# it has two parameters, the mean where the center of the peak is; the standard deviation is a measure of how spread out the data are
# the parameter mean and standard deviation of a normal distribution is different from the mean and standard deviation calculated from data

# check the distribution of data: compare empirical CDF to theoretical CDF
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

mean = np.mean(michelson_speed_of_light)
std = np.std(michelson_speed_of_light)
samples = np.random.normal(mean, std, size = 10000)
x, y = ecdf(michelson_speed_of_light)
x_theor, y_theor = ecdf(samples)

_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x,y, marker = '.', linestyle = 'none')
_ = plt.xlabel('speed of light (km/s)')
_ = plt.ylabel('CDF')
plt.show()

# plot  Normal PDF for various values of the variance using hacker statistics 
samples_std1 = np.random.normal(20, 1, size = 100000)
samples_std3 = np.random.normal(20, 3, size = 100000)
samples_std10= np.random.normal(20, 10, size= 100000)

# make histograms (PDF)
_ = plt.hist(samples_std1, normed=True, histtype = 'step', bins = 100)
_ = plt.hist(samples_std3, normed=True, histtype = 'step', bins = 100)
_ = plt.hist(samples_std10, normed=True, histtype = 'step', bins = 100)

# make a legend, set limits and show plot
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'))
plt.ylim(-0.01, 0.42)
plt.show()

# generate and plot the Normal CDFs
x_std1, y_std1 = ecdf(samples_std1)
x_std3, y_std3 = ecdf(samples_std3)
x_std10, y_std10 = ecdf(samples_std10)

_ = plt.plot(x_std1, y_std1, marker='.', linestyle = 'none')
_ = plt.plot(x_std3, y_std3, marker='.', linestyle = 'none')
_ = plt.plot(x_std10, y_std10, marker='.', linestyle = 'none')
_ = plt.legend(('std = 1', 'std = 3', 'std = 10'), loc = 'lower right')
plt.show()

# Normal distribution: f(x) = 1/sigma*sqrt(2*pi)  * exp(-(x-mu)**2/2*sigma**2)
# in practice, it is used to describe most symmetric peaked data you will encounter
# important caveats
# 1. often times things you may think are Normally distributed are not, to check, overlay the ECDF with a theoretical normal CDF
# 2. important issue about Normal distribution is the lightness of its tails. The probability of being more than 4 standard deviations 
# from the mean is very small. this means that when you are modeling data as Normally distributed, outliers are extremely unlikely.

# check if data is Normally distributed
mu = np.mean(belmont_no_outliers)
sigma = np.std(belmont_no_outliers)
samples = np.random.normal(mu, sigma, size = 10000)
x_theor, y_theor = ecdf(samples)
x,y = ecdf(belmont_no_outliers)
_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x,y, marker='.', linestyle = 'none')
_ = plt.xlabel('Belmont winning time (sec.)')
_ = plt.ylabel('CDF')
plt.show()

# The theoretical CDF and the ECDF of the data suggest that the winning Belmont times are, indeed, Normally distributed. 
# This also suggests that in the last 100 years or so, there have not been major technological or training advances that 
# have significantly affected the speed at which horses can run this race.

# what are the chances of a horse matching or beating the record of 144s
samples = np.random.normal(mu, sigma, size = 1000000)
prob = np.sum(samples <= 144)/1000000
print ('Probability of matching or beating the record is: ', prob)

# Exponential distribution: the amount of time between Poission process events
# parameter: the mean waiting time
# model the time between nuclear incidents (Poisson process)
mean = np.mean(inter_times)
samples = np.random.exponential(mean, size = 10000)
x, y = ecdf(inter_times)
x_theor, y_theor = ecdf(samples)
_ = plt.plot(x_theor, y_theor)
_ = plt.plot(x,y, marker = '.', linestyle = 'none')
_ = plt.xlabel('time (days)')
_ = plt.ylabel('CDF')
plt.show()

# simulate a story that does not have a named distribution to go along with it. 
# what's the total waiting time for the arrival of two different Poisson processes

def successive_poisson(tau1, tau2, size = 1):
    t1 = np.random.exponential(tau1, size)
    t2 = np.random.exponential(tau2, size)
    return t1 + t2
    
    #The mean waiting time for a no-hitter is 764 games, and the mean waiting time for hitting the cycle is 715 games.
    waiting_times = successive_poisson(764, 715, size = 100000)
    _ = plt.hist(waiting_times, bins = 100, histtype='step', normed=True)
    _ = plt.xlabel('Total waiting time (games)')
    _ = plt.ylabel('PDF')
    plt.show()
    
  
